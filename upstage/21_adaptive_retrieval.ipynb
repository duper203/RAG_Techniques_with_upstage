{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8ovbQHOe7ZtKMiPDK9jac",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duper203/RAG_Techniques_with_upstage/upstage/21__adaptive_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptive Retrieval-Augmented Generation (RAG) System\n",
        "\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. Query Classifier: Determines the type of query (Factual, Analytical, Opinion, or Contextual).\n",
        "\n",
        "2. Adaptive Retrieval Strategies: Four distinct strategies tailored to different query types:\n",
        "\n",
        "  * Factual Strategy\n",
        "  * Analytical Strategy\n",
        "  * Opinion Strategy\n",
        "  * Contextual Strategy\n",
        "3. LLM Integration: LLMs are used throughout the process to enhance retrieval and ranking.\n",
        "\n",
        "4. OpenAI GPT Model: Generates the final response using the retrieved documents as context.\n",
        "\n",
        "## Method Details\n",
        "\n",
        "1. Query Classification\n",
        "2. Adaptive Retrieval Strategies\n",
        "3. LLM-Enhanced Ranking\n",
        "4. Response Generation"
      ],
      "metadata": {
        "id": "_4vTJ_fSflby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install -qU langchain-upstage langchain langchain-community faiss-cpu PyMuPDF"
      ],
      "metadata": {
        "id": "MFxiQBMlWmjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import asyncio\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain_upstage import ChatUpstage, UpstageEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from typing import Dict, Any\n",
        "from langchain.docstore.document import Document\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "os.environ[\"UPSTAGE_API_KEY\"] = userdata.get(\"UPSTAGE_API_KEY\")"
      ],
      "metadata": {
        "id": "z_2vs2-ob4R3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class categories_options(BaseModel):\n",
        "        category: str = Field(description=\"The category of the query, the options are: Factual, Analytical, Opinion, or Contextual\", example=\"Factual\")\n",
        "\n",
        "class QueryClassifier:\n",
        "    def __init__(self):\n",
        "        self.llm = ChatUpstage()\n",
        "        self.prompt = PromptTemplate(\n",
        "            input_variables=[\"query\"],\n",
        "            template=\"Classify the following query into one of these categories: Factual, Analytical, Opinion, or Contextual.\\nQuery: {query}\\nCategory:\"\n",
        "        )\n",
        "        self.chain = self.prompt | self.llm.with_structured_output(categories_options)\n",
        "\n",
        "    def classify(self, query):\n",
        "        print(\"classifying query\")\n",
        "        return self.chain.invoke(query).category"
      ],
      "metadata": {
        "id": "kkYsUB5gS8wh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseRetrievalStrategy:\n",
        "    def __init__(self, texts):\n",
        "        self.embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
        "        text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=0)\n",
        "        self.documents = text_splitter.create_documents(texts)\n",
        "        self.db = FAISS.from_documents(self.documents, self.embeddings)\n",
        "        self.llm = ChatUpstage()\n",
        "\n",
        "    def retrieve(self, query, k=4):\n",
        "        return self.db.similarity_search(query, k=k)"
      ],
      "metadata": {
        "id": "F2Km7LonS-D4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class relevant_score(BaseModel):\n",
        "        score: float = Field(description=\"The relevance score of the document to the query\", example=8.0)\n",
        "\n",
        "class FactualRetrievalStrategy(BaseRetrievalStrategy):\n",
        "    def retrieve(self, query, k=4):\n",
        "        print(\"retrieving factual\")\n",
        "        enhanced_query_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\"],\n",
        "            template=\"Enhance this factual query for better information retrieval: {query}\"\n",
        "        )\n",
        "        query_chain = enhanced_query_prompt | self.llm\n",
        "        enhanced_query = query_chain.invoke(query).content\n",
        "        docs = self.db.similarity_search(enhanced_query, k=k*2)\n",
        "        ranked_docs = []\n",
        "        for doc in docs:\n",
        "            input_data = {\"query\": enhanced_query, \"doc\": doc.page_content}\n",
        "            score = float(ranking_chain.invoke(input_data).score)\n",
        "            ranked_docs.append((doc, score))\n",
        "\n",
        "        ranked_docs.sort(key=lambda x: x[1], reverse=True)\n",
        "        return [doc for doc, _ in ranked_docs[:k]]"
      ],
      "metadata": {
        "id": "eRy_8QcATArY"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}
