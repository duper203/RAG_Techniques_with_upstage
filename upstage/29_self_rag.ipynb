{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMr5J57MhL8N1sn7JYsLkgg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duper203/RAG_Techniques_with_upstage/blob/main/upstage/29_self_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-RAG: A Dynamic Approach to Retrieval-Augmented Generation\n",
        "\n",
        "## Key Components\n",
        "1. Retrieval Decision: Determines if retrieval is necessary for a given query.\n",
        "2. Document Retrieval: Fetches potentially relevant documents from a vector store.\n",
        "3. Relevance Evaluation: Assesses the relevance of retrieved documents to the query.\n",
        "4. Response Generation: Generates responses based on relevant contexts.\n",
        "5. Support Assessment: Evaluates how well the generated response is supported by the context.\n",
        "6. Utility Evaluation: Rates the usefulness of the generated response.\n"
      ],
      "metadata": {
        "id": "Lvw0QTte0AUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import relevant libraries &  helper_functions"
      ],
      "metadata": {
        "id": "6sHiscBy1C6b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BGMx_kQQheQS"
      },
      "outputs": [],
      "source": [
        "! pip3 install -qU langchain-upstage langchain langchain-community faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_upstage import UpstageEmbeddings, ChatUpstage\n",
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"UPSTAGE_API_KEY\"] = userdata.get(\"UPSTAGE_API_KEY\")\n"
      ],
      "metadata": {
        "id": "LmVEhyCshlWB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter  import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "def encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
        "\n",
        "    Args:\n",
        "        path: The path to the PDF file.\n",
        "        chunk_size: The desired size of each text chunk.\n",
        "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
        "\n",
        "    Returns:\n",
        "        A FAISS vector store containing the encoded book content.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load PDF documents\n",
        "    loader = PyPDFLoader(path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
        "    )\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    cleaned_texts = replace_t_with_space(texts)\n",
        "\n",
        "    # Create embeddings and vector store\n",
        "    embeddings =  UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
        "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
        "\n",
        "    return vectorstore\n",
        "def replace_t_with_space(list_of_documents):\n",
        "    \"\"\"\n",
        "    Replaces all tab characters ('\\t') with spaces in the page content of each document.\n",
        "\n",
        "    Args:\n",
        "        list_of_documents: A list of document objects, each with a 'page_content' attribute.\n",
        "\n",
        "    Returns:\n",
        "        The modified list of documents with tab characters replaced by spaces.\n",
        "    \"\"\"\n",
        "\n",
        "    for doc in list_of_documents:\n",
        "        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n",
        "    return list_of_documents\n"
      ],
      "metadata": {
        "id": "JPyTryUFvfUW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"data/Understanding_Climate_Change.pdf\"\n"
      ],
      "metadata": {
        "id": "aF4ALx6uv3vP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "w6cnknEc1I4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = encode_pdf(path)\n"
      ],
      "metadata": {
        "id": "rXuWUq9kv81l"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatUpstage()\n"
      ],
      "metadata": {
        "id": "UpWY11nyv-Jg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining prompt templates"
      ],
      "metadata": {
        "id": "gRaq31pz1IIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RetrievalResponse(BaseModel):\n",
        "    response: str = Field(..., title=\"Determines if retrieval is necessary\", description=\"Output only 'Yes' or 'No'.\")\n",
        "retrieval_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=\"Given the query '{query}', determine if retrieval is necessary. Output only 'Yes' or 'No'.\"\n",
        ")\n",
        "\n",
        "class RelevanceResponse(BaseModel):\n",
        "    response: str = Field(..., title=\"Determines if context is relevant\", description=\"Output only 'Relevant' or 'Irrelevant'.\")\n",
        "relevance_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"context\"],\n",
        "    template=\"Given the query '{query}' and the context '{context}', determine if the context is relevant. Output only 'Relevant' or 'Irrelevant'.\"\n",
        ")\n",
        "\n",
        "class GenerationResponse(BaseModel):\n",
        "    response: str = Field(..., title=\"Generated response\", description=\"The generated response.\")\n",
        "generation_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"context\"],\n",
        "    template=\"Given the query '{query}' and the context '{context}', generate a response.\"\n",
        ")\n",
        "\n",
        "class SupportResponse(BaseModel):\n",
        "    response: str = Field(..., title=\"Determines if response is supported\", description=\"Output 'Fully supported', 'Partially supported', or 'No support'.\")\n",
        "support_prompt = PromptTemplate(\n",
        "    input_variables=[\"response\", \"context\"],\n",
        "    template=\"Given the response '{response}' and the context '{context}', determine if the response is supported by the context. Output 'Fully supported', 'Partially supported', or 'No support'.\"\n",
        ")\n",
        "\n",
        "class UtilityResponse(BaseModel):\n",
        "    response: int = Field(..., title=\"Utility rating\", description=\"Rate the utility of the response from 1 to 5.\")\n",
        "utility_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"response\"],\n",
        "    template=\"Given the query '{query}' and the response '{response}', rate the utility of the response from 1 to 5.\"\n",
        ")\n",
        "\n",
        "# Create LLMChains for each step\n",
        "retrieval_chain = retrieval_prompt | llm.with_structured_output(RetrievalResponse)\n",
        "relevance_chain = relevance_prompt | llm.with_structured_output(RelevanceResponse)\n",
        "generation_chain = generation_prompt | llm.with_structured_output(GenerationResponse)\n",
        "support_chain = support_prompt | llm.with_structured_output(SupportResponse)\n",
        "utility_chain = utility_prompt | llm.with_structured_output(UtilityResponse)"
      ],
      "metadata": {
        "id": "Oysxo9KPv_zS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the self RAG logic flow"
      ],
      "metadata": {
        "id": "sr9D8zeu1KKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def self_rag(query, vectorstore, top_k=3):\n",
        "    print(f\"\\nProcessing query: {query}\")\n",
        "\n",
        "    # Step 1: Determine if retrieval is necessary\n",
        "    print(\"Step 1: Determining if retrieval is necessary...\")\n",
        "    input_data = {\"query\": query}\n",
        "    retrieval_decision = retrieval_chain.invoke(input_data).response.strip().lower()\n",
        "    print(f\"Retrieval decision: {retrieval_decision}\")\n",
        "\n",
        "    if retrieval_decision == 'yes':\n",
        "        # Step 2: Retrieve relevant documents\n",
        "        print(\"Step 2: Retrieving relevant documents...\")\n",
        "        docs = vectorstore.similarity_search(query, k=top_k)\n",
        "        contexts = [doc.page_content for doc in docs]\n",
        "        print(f\"Retrieved {len(contexts)} documents\")\n",
        "\n",
        "        # Step 3: Evaluate relevance of retrieved documents\n",
        "        print(\"Step 3: Evaluating relevance of retrieved documents...\")\n",
        "        relevant_contexts = []\n",
        "        for i, context in enumerate(contexts):\n",
        "            input_data = {\"query\": query, \"context\": context}\n",
        "            relevance = relevance_chain.invoke(input_data).response.strip().lower()\n",
        "            print(f\"Document {i+1} relevance: {relevance}\")\n",
        "            if relevance == 'relevant':\n",
        "                relevant_contexts.append(context)\n",
        "\n",
        "        print(f\"Number of relevant contexts: {len(relevant_contexts)}\")\n",
        "\n",
        "        # If no relevant contexts found, generate without retrieval\n",
        "        if not relevant_contexts:\n",
        "            print(\"No relevant contexts found. Generating without retrieval...\")\n",
        "            input_data = {\"query\": query, \"context\": \"No relevant context found.\"}\n",
        "            return generation_chain.invoke(input_data).response\n",
        "\n",
        "        # Step 4: Generate response using relevant contexts\n",
        "        print(\"Step 4: Generating responses using relevant contexts...\")\n",
        "        responses = []\n",
        "        for i, context in enumerate(relevant_contexts):\n",
        "            print(f\"Generating response for context {i+1}...\")\n",
        "            input_data = {\"query\": query, \"context\": context}\n",
        "            response = generation_chain.invoke(input_data).response\n",
        "\n",
        "            # Step 5: Assess support\n",
        "            print(f\"Step 5: Assessing support for response {i+1}...\")\n",
        "            input_data = {\"response\": response, \"context\": context}\n",
        "            support = support_chain.invoke(input_data).response.strip().lower()\n",
        "            print(f\"Support assessment: {support}\")\n",
        "\n",
        "            # Step 6: Evaluate utility\n",
        "            print(f\"Step 6: Evaluating utility for response {i+1}...\")\n",
        "            input_data = {\"query\": query, \"response\": response}\n",
        "            utility = int(utility_chain.invoke(input_data).response)\n",
        "            print(f\"Utility score: {utility}\")\n",
        "\n",
        "            responses.append((response, support, utility))\n",
        "\n",
        "        # Select the best response based on support and utility\n",
        "        print(\"Selecting the best response...\")\n",
        "        best_response = max(responses, key=lambda x: (x[1] == 'fully supported', x[2]))\n",
        "        print(f\"Best response support: {best_response[1]}, utility: {best_response[2]}\")\n",
        "        return best_response[0]\n",
        "    else:\n",
        "        # Generate without retrieval\n",
        "        print(\"Generating without retrieval...\")\n",
        "        input_data = {\"query\": query, \"context\": \"No retrieval necessary.\"}\n",
        "        return generation_chain.invoke(input_data).response"
      ],
      "metadata": {
        "id": "C-YJLP9BwEBL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the self-RAG function easy query with high relevance"
      ],
      "metadata": {
        "id": "1PDXNX3K1MJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the impact of climate change on the environment?\"\n",
        "response = self_rag(query, vectorstore)\n",
        "\n",
        "print(\"\\nFinal response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeMvzhGwwGAc",
        "outputId": "3eefb267-148b-4b86-e05f-cacfa4aa7ee2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing query: What is the impact of climate change on the environment?\n",
            "Step 1: Determining if retrieval is necessary...\n",
            "Retrieval decision: yes\n",
            "Step 2: Retrieving relevant documents...\n",
            "Retrieved 3 documents\n",
            "Step 3: Evaluating relevance of retrieved documents...\n",
            "Document 1 relevance: relevant\n",
            "Document 2 relevance: relevant\n",
            "Document 3 relevance: relevant\n",
            "Number of relevant contexts: 3\n",
            "Step 4: Generating responses using relevant contexts...\n",
            "Generating response for context 1...\n",
            "Step 5: Assessing support for response 1...\n",
            "Support assessment: fully supported\n",
            "Step 6: Evaluating utility for response 1...\n",
            "Utility score: 5\n",
            "Generating response for context 2...\n",
            "Step 5: Assessing support for response 2...\n",
            "Support assessment: climate change is altering the timing and length of seasons, affecting ecosystems and human activities. for example, spring is arriving earlier, and winters are becoming shorter and milder in many regions. this shift disrupts plant and animal life cycles and agricultural practices. warmer temperatures are causing polar ice caps and glaciers to melt, contributing to rising sea levels. sea levels have risen by about 20 centimeters (8 inches) in the past century, threatening coastal communities and ecosystems.\n",
            "Step 6: Evaluating utility for response 2...\n",
            "Utility score: 5\n",
            "Generating response for context 3...\n",
            "Step 5: Assessing support for response 3...\n",
            "Support assessment: climate change has a significant impact on the environment and can lead to numerous consequences. extreme weather events, such as hurricanes, heatwaves, droughts, and heavy rainfall, are becoming more frequent and severe. these events can have devastating impacts on communities, economies, and ecosystems. for example, hurricanes and typhoons can intensify, leading to more destructive storms, while droughts can affect agriculture, water supply, and ecosystems. flooding is also becoming more common due to heavy rainfall events. early warning systems and resilient infrastructure are critical for mitigating these risks.\n",
            "Step 6: Evaluating utility for response 3...\n",
            "Utility score: 4\n",
            "Selecting the best response...\n",
            "Best response support: fully supported, utility: 5\n",
            "\n",
            "Final response:\n",
            "Climate change is having a significant impact on the environment. The Arctic is warming at more than twice the global average rate, leading to significant ice loss. Antarctic ice sheets are also losing mass, contributing to sea level rise. This melting affects global ocean currents and weather patterns. Glaciers around the world are retreating, affecting water supplies for millions of people, especially in regions like the Himalayas and the Andes. Coastal erosion is accelerating, threatening homes, infrastructure, and ecosystems, especially in low-lying islands and coastal regions. Climate change is also linked to an increase in the frequency and severity of extreme weather events.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the self-RAG function with a more challenging query with low relevance"
      ],
      "metadata": {
        "id": "XPirR1z61Nn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"how did harry beat quirrell?\"\n",
        "response = self_rag(query, vectorstore)\n",
        "\n",
        "print(\"\\nFinal response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUGYtwcvwGXM",
        "outputId": "106b235f-a8ce-4a8d-b4cb-32efcda8406a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing query: how did harry beat quirrell?\n",
            "Step 1: Determining if retrieval is necessary...\n",
            "Retrieval decision: no\n",
            "Generating without retrieval...\n",
            "\n",
            "Final response:\n",
            "When Harry Potter discovered that Professor Quirrell was possessed by Voldemort, he managed to defeat them both. Quirrell, who was not truly a wizard, was unable to withstand Voldemort's power and disintegrated. Harry was able to escape from the Chamber of Secrets by using the Sorting Hat to summon the Sword of Gryffindor, which he used to kill the Basilisk. He then used the Sorting Hat to escape from the Chamber and return to the castle.\n"
          ]
        }
      ]
    }
  ]
}