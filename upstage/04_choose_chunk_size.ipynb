{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duper203/RAG_Techniques_with_upstage/blob/main/upstage/04_choose_chunk_size.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emWlJR1aOIWs"
      },
      "source": [
        "# Choose Chunk Size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbxwK8wEPUB4"
      },
      "source": [
        "### Import libraries and environment variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guaAiLcXGDb5"
      },
      "outputs": [],
      "source": [
        "! pip3 install -qU llama-index-llms-langchain langchain_community langchain_upstage llama_index llama-index-llms-upstage llama-index-embeddings-upstage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZp3g-glF4LP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"UPSTAGE_API_KEY\"] = userdata.get(\"UPSTAGE_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "\n",
        "\n",
        "import nest_asyncio\n",
        "import random\n",
        "\n",
        "nest_asyncio.apply()\n",
        "# from dotenv import load_dotenv\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "\n",
        "from llama_index.core.evaluation import (\n",
        "    DatasetGenerator,\n",
        "    FaithfulnessEvaluator,\n",
        "    RelevancyEvaluator\n",
        ")\n",
        "\n",
        "from llama_index.llms.openai import OpenAI\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db60yaACx3np"
      },
      "source": [
        "## Read Docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S33FioCnexSQ"
      },
      "outputs": [],
      "source": [
        "data_dir = \"data\"\n",
        "documents = SimpleDirectoryReader(data_dir).load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kx6eAt8x6VV"
      },
      "source": [
        "## Create evaluation questions and pick k out of them\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw92uZ3AqGih",
        "outputId": "25daf5d6-9131-437c-c60b-b24f6da0ba54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['13. What is the relationship between increased CO2 levels in the atmosphere and ocean acidification?', '6. How does engaging local communities in restoration projects contribute to their sustainability and long-term success?', '5. How do scientists use ice core samples, tree rings, and ocean sediments to study climate change?', '10. How can balancing energy needs with ecological conservation be achieved in the context of hydroelectric power?', '7. What are some effective measures for reducing emissions from vehicles, industries, and power plants to improve air quality and public health?', '4. What are the benefits and challenges of transitioning to electric vehicles on a large scale?', '1. What are some examples of indigenous practices that contribute to sustainable land and resource management?', '9. What are the three main types of fossil fuels mentioned in the document?', '8. How can youth engagement in climate action be further empowered and supported?', '8. What are some best practices for tailoring climate communication messages to different audiences?', '8. How do warmer ocean temperatures intensify hurricanes and typhoons?', '3. How does climate-smart healthcare improve both health outcomes and environmental sustainability?', '1. What are some examples of large-scale climate solutions that can be implemented through public-private partnerships (PPPs)?', '10. How can sustainable land use practices, such as agroforestry and regenerative agriculture, help sequester carbon and support resilient ecosystems?', '14. How can the carbon sequestration potential of reforestation and afforestation projects be maximized?', '1. What are some specific examples of smart building technologies that can improve energy efficiency in buildings?', '7. How can youth-led initiatives, such as school strikes for climate, be supported and amplified to maximize their impact?', '5. How can blockchain technology enhance transparency and accountability in climate action efforts?', '7. What are the environmental considerations when implementing hydroelectric power projects?', '11. What role do national policies play in implementing climate mitigation and adaptation strategies, and what are some examples of effective national policies?', '11. Can you explain the concept of agroforestry and its benefits for sustainable agriculture?', '7. What is direct air capture technology, and how does it contribute to achieving net-zero emissions?', '5. How does the use of synthetic fertilizers release nitrous oxide, and what are the benefits of precision farming and organic fertilizers in mitigating these emissions?', '6. How does climate change exacerbate air pollution, and what health conditions are affected as a result?', '4. How does climate justice address the unequal distribution of climate change impacts among vulnerable populations?']\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_upstage import ChatUpstage\n",
        "\n",
        "# Initialize the Upstage LLM client\n",
        "llm_langchain = ChatUpstage(model='solar-pro')\n",
        "\n",
        "# Define a prompt for generating questions\n",
        "prompt_template = \"\"\"\n",
        "You are given a document. Your task is to generate a set of evaluation questions based on the content of the document.\n",
        "\n",
        "Document:\n",
        "{document_text}\n",
        "\n",
        "Generate a list of questions based on this document:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"document_text\"])\n",
        "\n",
        "# Initialize an LLM chain\n",
        "llm_chain = LLMChain(llm=llm_langchain, prompt=prompt)\n",
        "\n",
        "# Use the first 20 documents for evaluation\n",
        "eval_documents = documents[0:20]\n",
        "eval_questions = []\n",
        "\n",
        "# Generate questions from each document\n",
        "for document in eval_documents:\n",
        "    document_text = document.text  # Assuming document.text holds the text data\n",
        "    questions = llm_chain.run(document_text)\n",
        "    eval_questions.extend(questions.split(\"\\n\"))\n",
        "\n",
        "# Select k random evaluation questions\n",
        "num_eval_questions = 25\n",
        "k_eval_questions = random.sample(eval_questions, num_eval_questions)\n",
        "\n",
        "# Print or use the evaluation questions\n",
        "print(k_eval_questions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnGq_uRqlyFH"
      },
      "source": [
        "## Define metrics evaluators and modify llama_index faithfullness evaluator prompt to rely on the context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xC_g0aRLl9L1"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
        "from llama_index.core import Settings\n",
        "\n",
        "gpt4 = OpenAI(temperature=0, model=\"gpt-4o\")\n",
        "\n",
        "# Define Faithfulness and Relevancy Evaluators which are based on GPT-4\n",
        "faithfulness_gpt4 = FaithfulnessEvaluator(llm=gpt4)\n",
        "\n",
        "faithfulness_new_prompt_template = PromptTemplate(\n",
        "    template=\"\"\" Please tell if a given piece of information is directly supported by the context.\n",
        "    You need to answer with either YES or NO.\n",
        "    Answer YES if any part of the context explicitly supports the information, even if most of the context is unrelated. If the context does not explicitly support the information, answer NO. Some examples are provided below.\n",
        "\n",
        "    Information: Apple pie is generally double-crusted.\n",
        "    Context: An apple pie is a fruit pie in which the principal filling ingredient is apples.\n",
        "    Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard, or cheddar cheese.\n",
        "    It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
        "    Answer: YES\n",
        "\n",
        "    Information: Apple pies taste bad.\n",
        "    Context: An apple pie is a fruit pie in which the principal filling ingredient is apples.\n",
        "    Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard, or cheddar cheese.\n",
        "    It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
        "    Answer: NO\n",
        "\n",
        "    Information: Paris is the capital of France.\n",
        "    Context: This document describes a day trip in Paris. You will visit famous landmarks like the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\n",
        "    Answer: NO\n",
        "\n",
        "    Information: {query_str}\n",
        "    Context: {context_str}\n",
        "    Answer:\n",
        "\n",
        "    \"\"\")\n",
        "\n",
        "faithfulness_gpt4.update_prompts({\"your_prompt_key\": faithfulness_new_prompt_template}) # Update the prompts dictionary with the new prompt template\n",
        "relevancy_gpt4 = RelevancyEvaluator(llm=gpt4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDI6gzuBm2tK"
      },
      "source": [
        "## Function to evaluate metrics for each chunk size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgNTLdvIvE3J"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.upstage import Upstage\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.embeddings.upstage import UpstageEmbedding\n",
        "\n",
        "embed_model = UpstageEmbedding(model=\"solar-embedding-1-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "23O-vyyCm4Zo"
      },
      "outputs": [],
      "source": [
        "# Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size\n",
        "# Generate response and evaluate it.\n",
        "from langchain_upstage import UpstageEmbeddings\n",
        "def evaluate_response_time_and_accuracy(chunk_size, eval_questions):\n",
        "    \"\"\"\n",
        "    Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for a given chunk size.\n",
        "\n",
        "    Parameters:\n",
        "    chunk_size (int): The size of data chunks being processed.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    total_response_time = 0\n",
        "    total_faithfulness = 0\n",
        "    total_relevancy = 0\n",
        "\n",
        "    # service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size, chunk_overlap=chunk_size//5)\n",
        "    Settings.llm = ChatUpstage(model='solar-pro')\n",
        "    Settings.embed_model = embed_model\n",
        "    Settings.chunk_size = chunk_size\n",
        "    Settings.chunk_overlap = chunk_size//5\n",
        "\n",
        "    vector_index = VectorStoreIndex.from_documents(\n",
        "        eval_documents, embed_model=Settings.embed_model\n",
        "    )\n",
        "    # build query engine\n",
        "    query_engine = vector_index.as_query_engine(similarity_top_k=5)\n",
        "    num_questions = len(eval_questions)\n",
        "\n",
        "    # Iterate over each question in eval_questions to compute metrics.\n",
        "    # While BatchEvalRunner can be used for faster evaluations (see: https://docs.llamaindex.ai/en/latest/examples/evaluation/batch_eval.html),\n",
        "    # we're using a loop here to specifically measure response time for different chunk sizes.\n",
        "    for question in eval_questions:\n",
        "        start_time = time.time()\n",
        "        response_vector = query_engine.query(question)\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        faithfulness_result = faithfulness_gpt4.evaluate_response(\n",
        "            response=response_vector\n",
        "        ).passing\n",
        "\n",
        "        relevancy_result = relevancy_gpt4.evaluate_response(\n",
        "            query=question, response=response_vector\n",
        "        ).passing\n",
        "\n",
        "        total_response_time += elapsed_time\n",
        "        total_faithfulness += faithfulness_result\n",
        "        total_relevancy += relevancy_result\n",
        "\n",
        "    average_response_time = total_response_time / num_questions\n",
        "    average_faithfulness = total_faithfulness / num_questions\n",
        "    average_relevancy = total_relevancy / num_questions\n",
        "\n",
        "    return average_response_time, average_faithfulness, average_relevancy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qLQXfF5nCJZ"
      },
      "source": [
        "## Test different chunk sizes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JFfU_IqTnBfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "593ef3e8-818b-48ca-fa7d-02caf2e62404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk size 128 - Average Response time: 3.65s, Average Faithfulness: 1.00, Average Relevancy: 1.00\n",
            "Chunk size 256 - Average Response time: 3.92s, Average Faithfulness: 1.00, Average Relevancy: 1.00\n"
          ]
        }
      ],
      "source": [
        "chunk_sizes = [128, 256]\n",
        "\n",
        "for chunk_size in chunk_sizes:\n",
        "  avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size, k_eval_questions)\n",
        "  print(f\"Chunk size {chunk_size} - Average Response time: {avg_response_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVPmUW/4PnIXLxawxIZ0A6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}