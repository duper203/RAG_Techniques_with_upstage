{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTbitKwwXan8f+myIubHWX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duper203/RAG_Techniques_with_upstage/blob/main/upstage/19_1_multi_model_rag_with_captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi Model RAG with captioning\n",
        "\n",
        "This code implements one of the multiple ways of multi-model RAG. It extracts and processes text and images from PDFs, utilizing a multi-modal Retrieval-Augmented Generation (RAG) system for summarizing and retrieving content for question answering.\n",
        "\n",
        "\n",
        "\n",
        "## Key Components\n",
        "1. PyMuPDF: For extracting text and images from PDFs.\n",
        "2. Gemini 1.5-flash model: To summarize images and tables.\n",
        "3. Upstage Embeddings: For embedding document splits.\n",
        "4. Chroma Vectorstore: To store and retrieve document embeddings.\n",
        "5. LangChain: To orchestrate the retrieval and generation pipeline."
      ],
      "metadata": {
        "id": "GJCQEVKx29LQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q_fpd88Moi6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import relevant libraries"
      ],
      "metadata": {
        "id": "V6S1n7kf-GHk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BGMx_kQQheQS"
      },
      "outputs": [],
      "source": [
        "! pip3 install -qU langchain-upstage langchain langchain-community chromadb PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from PIL import Image\n",
        "import io\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "import google.generativeai as genai\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.documents import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_upstage import ChatUpstage, UpstageEmbeddings\n",
        "os.environ[\"UPSTAGE_API_KEY\"] = userdata.get(\"UPSTAGE_API_KEY\")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")"
      ],
      "metadata": {
        "id": "Ki52G5T5Htbi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the \"Attention is all you need\" paper"
      ],
      "metadata": {
        "id": "zmZSUPXTokAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://arxiv.org/pdf/1706.03762\n",
        "!mv 1706.03762 attention_is_all_you_need.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ByrMALulcJJ",
        "outputId": "2e1bf9ee-1267-4138-8fa4-6c1eef55d27d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-22 23:35:22--  https://arxiv.org/pdf/1706.03762\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.195.42, 151.101.131.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2215244 (2.1M) [application/pdf]\n",
            "Saving to: ‘1706.03762’\n",
            "\n",
            "1706.03762          100%[===================>]   2.11M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-10-22 23:35:22 (30.7 MB/s) - ‘1706.03762’ saved [2215244/2215244]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Extraction"
      ],
      "metadata": {
        "id": "0dAgToTDomR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = []\n",
        "img_data = []"
      ],
      "metadata": {
        "id": "UQi4PwAuldhu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with fitz.open('attention_is_all_you_need.pdf') as pdf_file:\n",
        "    # Create a directory to store the images\n",
        "    if not os.path.exists(\"extracted_images\"):\n",
        "        os.makedirs(\"extracted_images\")\n",
        "\n",
        "    # Loop through every page in the PDF\n",
        "    for page_number in range(len(pdf_file)):\n",
        "        page = pdf_file[page_number]\n",
        "\n",
        "        # Get the text on page\n",
        "        text = page.get_text().strip()\n",
        "        text_data.append({\"response\": text, \"name\": page_number+1})\n",
        "        # Get the list of images on the page\n",
        "        images = page.get_images(full=True)\n",
        "\n",
        "        # Loop through all images found on the page\n",
        "        for image_index, img in enumerate(images, start=0):\n",
        "            xref = img[0]  # Get the XREF of the image\n",
        "            base_image = pdf_file.extract_image(xref)  # Extract the image\n",
        "            image_bytes = base_image[\"image\"]  # Get the image bytes\n",
        "            image_ext = base_image[\"ext\"]  # Get the image extension\n",
        "\n",
        "            # Load the image using PIL and save it\n",
        "            image = Image.open(io.BytesIO(image_bytes))\n",
        "            image.save(f\"extracted_images/image_{page_number+1}_{image_index+1}.{image_ext}\")"
      ],
      "metadata": {
        "id": "eVgwEuFYle5Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "3-dDcXs7lg7i"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Captioning"
      ],
      "metadata": {
        "id": "Z4pTRaJ5ook-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for img in os.listdir(\"extracted_images\"):\n",
        "    image = Image.open(f\"extracted_images/{img}\")\n",
        "    response = model.generate_content([image, \"You are an assistant tasked with summarizing tables, images and text for retrieval. \\\n",
        "    These summaries will be embedded and used to retrieve the raw text or table elements \\\n",
        "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text or image:\"])\n",
        "    img_data.append({\"response\": response.text, \"name\": img})"
      ],
      "metadata": {
        "id": "KHBRyUbIliR_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enI6WsVTnPl1",
        "outputId": "32206d3a-c580-41bb-ed95-32a22f5d7900"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'response': 'This image shows the architecture of a scaled dot-product attention mechanism. The input is three tensors V, K, and Q, which are first linearly transformed. Then, the transformed K and Q tensors are multiplied with each other. The resulting product is then normalized by the dimension of the Q tensor and scaled down by a factor of the square root of the dimension. Finally, the result is multiplied with the transformed V tensor to produce the final output.',\n",
              "  'name': 'image_4_2.png'},\n",
              " {'response': 'This image shows the steps of the scaled dot-product attention mechanism used in Transformers.  Q, K, and V are inputs that are multiplied with each other and then scaled to ensure that the gradients are well behaved.  The resulting output is masked and then fed into a softmax layer to calculate the attention weights.  The weighted values are then multiplied by V to get the final output.',\n",
              "  'name': 'image_4_1.png'},\n",
              " {'response': 'This is a diagram of the Transformer architecture, a neural network architecture used for tasks like machine translation and text summarization. The diagram shows how the input is processed through a series of layers, including embedding, positional encoding, multi-head attention, and feedforward networks. The output is then produced through a softmax layer.',\n",
              "  'name': 'image_3_1.png'},\n",
              " {'response': 'This image illustrates the scaled dot-product attention mechanism used in transformer models. It involves three inputs: V, K, and Q (value, key, and query). Each input is transformed by a linear layer. The outputs from these linear layers are then used in the dot-product attention mechanism, which calculates the attention weights. Finally, a concatenation operation is performed, followed by another linear layer.  The output of the final linear layer is the output of the scaled dot-product attention.',\n",
              "  'name': 'image_4_2.png'},\n",
              " {'response': 'This image is a diagram showing the steps involved in self-attention. The first step is to perform a matrix multiplication on the query (Q), key (K) and value (V) matrices. The result of this multiplication is then scaled. The scaled result is then masked to ensure that the model only attends to relevant parts of the input. Finally, a softmax function is applied to the masked result to produce the attention weights.  These weights are then used to create a weighted sum of the values, which is the output of the self-attention layer.',\n",
              "  'name': 'image_4_1.png'},\n",
              " {'response': \"This image shows the architecture of a Transformer model, which is a type of neural network used for natural language processing tasks. The model consists of an encoder and a decoder. The encoder takes in a sequence of words and produces a representation of that sequence. The decoder then takes the encoder's representation and generates a new sequence of words. The model is trained to predict the next word in a sequence given the previous words, and it can be used for tasks such as machine translation and text summarization. The image shows the components of a Transformer model and their connections. The model consists of multiple layers, including multi-head attention layers, feed-forward layers, and normalization layers. The image also shows how the model processes input and output sequences.\",\n",
              "  'name': 'image_3_1.png'}]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorstore"
      ],
      "metadata": {
        "id": "nupiGhSmoruT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set embeddings\n",
        "embedding_model = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
        "\n",
        "# Load the document\n",
        "docs_list = [Document(page_content=text['response'], metadata={\"name\": text['name']}) for text in text_data]\n",
        "img_list = [Document(page_content=img['response'], metadata={\"name\": img['name']}) for img in img_data]\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=400, chunk_overlap=50\n",
        ")\n",
        "\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "img_splits = text_splitter.split_documents(img_list)"
      ],
      "metadata": {
        "id": "ESDtiHfQllO-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add to vectorstore\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits + img_splits, # adding the both text and image splits\n",
        "    collection_name=\"multi_model_rag\",\n",
        "    embedding=embedding_model,\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever(\n",
        "                search_type=\"similarity\",\n",
        "                search_kwargs={'k': 1}, # number of documents to retrieve\n",
        "            )"
      ],
      "metadata": {
        "id": "jDIKdmGclqGE"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query"
      ],
      "metadata": {
        "id": "EUzetKK-ot8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is a Transformer model?\""
      ],
      "metadata": {
        "id": "_v31A9kImG82"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = retriever.invoke(query)"
      ],
      "metadata": {
        "id": "zCw2qDEtmHv8"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output"
      ],
      "metadata": {
        "id": "f7LGuOh0ovmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are an assistant for question-answering tasks. Answer the question based upon your knowledge.\n",
        "Use three-to-five sentences maximum and keep the answer concise.\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Retrieved documents: \\n\\n <docs>{documents}</docs> \\n\\n User question: <question>{question}</question>\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# LLM\n",
        "llm = ChatUpstage(model='solar-pro')\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run\n",
        "generation = rag_chain.invoke({\"documents\":docs[0].page_content, \"question\": query})\n",
        "print(generation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lJqJ-pOmKNJ",
        "outputId": "3c61fee4-d209-4437-96b9-0374ffa444e2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Transformer model is a type of neural network used for natural language processing tasks. It has an encoder-decoder architecture, where the encoder creates a representation of an input sequence and the decoder generates a new sequence based on that representation. The model consists of multiple layers, including attention and feed-forward layers, and is trained to predict the next word in a sequence. Transformer models are commonly used for machine translation and text summarization.\n"
          ]
        }
      ]
    }
  ]
}