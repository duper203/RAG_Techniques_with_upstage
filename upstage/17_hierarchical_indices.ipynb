{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTVK6dCBxCiuFkb0qcnUa6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duper203/RAG_Techniques_with_upstage/blob/main/upstage/17_hierarchical_indices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchical Indices in Document Retrieval\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Key Components\n",
        "\n",
        "Reranking systems typically include the following components:\n",
        "\n",
        "\n",
        "1. PDF processing and text chunking\n",
        "2. Asynchronous document summarization using Upstage Solar\n",
        "3. Vector store creation for both summaries and detailed chunks using FAISS and Upstage embeddings\n",
        "4. Custom hierarchical retrieval function\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Method Details\n",
        "\n",
        "1. Document Preprocessing and Encoding\n",
        "\n",
        "\n",
        "2. Asynchronous Processing and Rate Limiting\n",
        "\n",
        "\n",
        "3. Hierarchical Retrieval"
      ],
      "metadata": {
        "id": "_4vTJ_fSflby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install -qU langchain-upstage langchain langchain-community faiss-cpu sentence_transformers"
      ],
      "metadata": {
        "id": "MFxiQBMlWmjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import asyncio\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain_upstage import UpstageEmbeddings, ChatUpstage\n",
        "from langchain.chains.summarize.chain import load_summarize_chain\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "os.environ[\"UPSTAGE_API_KEY\"] = userdata.get(\"UPSTAGE_API_KEY\")"
      ],
      "metadata": {
        "id": "z_2vs2-ob4R3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define document(s) path & Read PDf to string"
      ],
      "metadata": {
        "id": "DhaAjrWHY20s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"data/Understanding_Climate_Change.pdf\""
      ],
      "metadata": {
        "id": "fSlBtO2-UuAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "async def exponential_backoff(attempt):\n",
        "    \"\"\"\n",
        "    Implements exponential backoff with a jitter.\n",
        "\n",
        "    Args:\n",
        "        attempt: The current retry attempt number.\n",
        "\n",
        "    Waits for a period of time before retrying the operation.\n",
        "    The wait time is calculated as (2^attempt) + a random fraction of a second.\n",
        "    \"\"\"\n",
        "    # Calculate the wait time with exponential backoff and jitter\n",
        "    wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
        "    print(f\"Rate limit hit. Retrying in {wait_time:.2f} seconds...\")\n",
        "\n",
        "    # Asynchronously sleep for the calculated wait time\n",
        "    await asyncio.sleep(wait_time)\n",
        "\n",
        "async def retry_with_exponential_backoff(coroutine, max_retries=5):\n",
        "    \"\"\"\n",
        "    Retries a coroutine using exponential backoff upon encountering a RateLimitError.\n",
        "\n",
        "    Args:\n",
        "        coroutine: The coroutine to be executed.\n",
        "        max_retries: The maximum number of retry attempts.\n",
        "\n",
        "    Returns:\n",
        "        The result of the coroutine if successful.\n",
        "\n",
        "    Raises:\n",
        "        The last encountered exception if all retry attempts fail.\n",
        "    \"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # Attempt to execute the coroutine\n",
        "            return await coroutine\n",
        "        except RateLimitError as e:\n",
        "            # If the last attempt also fails, raise the exception\n",
        "            if attempt == max_retries - 1:\n",
        "                raise e\n",
        "\n",
        "            # Wait for an exponential backoff period before retrying\n",
        "            await exponential_backoff(attempt)\n",
        "\n",
        "    # If max retries are reached without success, raise an exception\n",
        "    raise Exception(\"Max retries reached\")\n",
        "\n",
        "async def encode_pdf_hierarchical(path, chunk_size=1000, chunk_overlap=200, is_string=False):\n",
        "    \"\"\"\n",
        "    Asynchronously encodes a PDF book into a hierarchical vector store using OpenAI embeddings.\n",
        "    Includes rate limit handling with exponential backoff.\n",
        "\n",
        "    Args:\n",
        "        path: The path to the PDF file.\n",
        "        chunk_size: The desired size of each text chunk.\n",
        "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing two FAISS vector stores:\n",
        "        1. Document-level summaries\n",
        "        2. Detailed chunks\n",
        "    \"\"\"\n",
        "\n",
        "    # Load PDF documents\n",
        "    if not is_string:\n",
        "        loader = PyPDFLoader(path)\n",
        "        documents = await asyncio.to_thread(loader.load)\n",
        "    else:\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            # Set a really small chunk size, just to show.\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "            is_separator_regex=False,\n",
        "        )\n",
        "        documents = text_splitter.create_documents([path])\n",
        "\n",
        "\n",
        "    # Create document-level summaries\n",
        "    summary_llm = ChatUpstage(model=\"solar-pro\")\n",
        "    summary_chain = load_summarize_chain(summary_llm, chain_type=\"map_reduce\")\n",
        "\n",
        "    async def summarize_doc(doc):\n",
        "        \"\"\"\n",
        "        Summarizes a single document with rate limit handling.\n",
        "\n",
        "        Args:\n",
        "            doc: The document to be summarized.\n",
        "\n",
        "        Returns:\n",
        "            A summarized Document object.\n",
        "        \"\"\"\n",
        "        # Retry the summarization with exponential backoff\n",
        "        summary_output = await retry_with_exponential_backoff(summary_chain.ainvoke([doc]))\n",
        "        summary = summary_output['output_text']\n",
        "        return Document(\n",
        "            page_content=summary,\n",
        "            metadata={\"source\": path, \"page\": doc.metadata[\"page\"], \"summary\": True}\n",
        "        )\n",
        "\n",
        "    # Process documents in smaller batches to avoid rate limits\n",
        "    batch_size = 5  # Adjust this based on your rate limits\n",
        "    summaries = []\n",
        "    for i in range(0, len(documents), batch_size):\n",
        "        batch = documents[i:i+batch_size]\n",
        "        batch_summaries = await asyncio.gather(*[summarize_doc(doc) for doc in batch])\n",
        "        summaries.extend(batch_summaries)\n",
        "        await asyncio.sleep(1)  # Short pause between batches\n",
        "\n",
        "    # Split documents into detailed chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
        "    )\n",
        "    detailed_chunks = await asyncio.to_thread(text_splitter.split_documents, documents)\n",
        "\n",
        "    # Update metadata for detailed chunks\n",
        "    for i, chunk in enumerate(detailed_chunks):\n",
        "        chunk.metadata.update({\n",
        "            \"chunk_id\": i,\n",
        "            \"summary\": False,\n",
        "            \"page\": int(chunk.metadata.get(\"page\", 0))\n",
        "        })\n",
        "\n",
        "    # Create embeddings\n",
        "    embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
        "\n",
        "    # Create vector stores asynchronously with rate limit handling\n",
        "    async def create_vectorstore(docs):\n",
        "        \"\"\"\n",
        "        Creates a vector store from a list of documents with rate limit handling.\n",
        "\n",
        "        Args:\n",
        "            docs: The list of documents to be embedded.\n",
        "\n",
        "        Returns:\n",
        "            A FAISS vector store containing the embedded documents.\n",
        "        \"\"\"\n",
        "        return await retry_with_exponential_backoff(\n",
        "            asyncio.to_thread(FAISS.from_documents, docs, embeddings)\n",
        "        )\n",
        "\n",
        "    # Generate vector stores for summaries and detailed chunks concurrently\n",
        "    summary_vectorstore, detailed_vectorstore = await asyncio.gather(\n",
        "        create_vectorstore(summaries),\n",
        "        create_vectorstore(detailed_chunks)\n",
        "    )\n",
        "\n",
        "    return summary_vectorstore, detailed_vectorstore"
      ],
      "metadata": {
        "id": "X0dgwNPOwg80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encode the PDF book to both document-level summaries and detailed chunks if the vector stores do not exist"
      ],
      "metadata": {
        "id": "3hO7L7zKxy7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "if os.path.exists(\"../vector_stores/summary_store\") and os.path.exists(\"../vector_stores/detailed_store\"):\n",
        "   embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
        "   summary_store = FAISS.load_local(\"../vector_stores/summary_store\", embeddings, allow_dangerous_deserialization=True)\n",
        "   detailed_store = FAISS.load_local(\"../vector_stores/detailed_store\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "else:\n",
        "    summary_store, detailed_store = await encode_pdf_hierarchical(path)\n",
        "    summary_store.save_local(\"../vector_stores/summary_store\")\n",
        "    detailed_store.save_local(\"../vector_stores/detailed_store\")"
      ],
      "metadata": {
        "id": "HkDQ991XxrKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve information according to summary level, and then retrieve information from the chunk level vector store and filter according to the summary level pages"
      ],
      "metadata": {
        "id": "PkqcH70Yx3IY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_hierarchical(query, summary_vectorstore, detailed_vectorstore, k_summaries=3, k_chunks=5):\n",
        "    \"\"\"\n",
        "    Performs a hierarchical retrieval using the query.\n",
        "\n",
        "    Args:\n",
        "        query: The search query.\n",
        "        summary_vectorstore: The vector store containing document summaries.\n",
        "        detailed_vectorstore: The vector store containing detailed chunks.\n",
        "        k_summaries: The number of top summaries to retrieve.\n",
        "        k_chunks: The number of detailed chunks to retrieve per summary.\n",
        "\n",
        "    Returns:\n",
        "        A list of relevant detailed chunks.\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve top summaries\n",
        "    top_summaries = summary_vectorstore.similarity_search(query, k=k_summaries)\n",
        "\n",
        "    relevant_chunks = []\n",
        "    for summary in top_summaries:\n",
        "        # For each summary, retrieve relevant detailed chunks\n",
        "        page_number = summary.metadata[\"page\"]\n",
        "        page_filter = lambda metadata: metadata[\"page\"] == page_number\n",
        "        page_chunks = detailed_vectorstore.similarity_search(\n",
        "            query,\n",
        "            k=k_chunks,\n",
        "            filter=page_filter\n",
        "        )\n",
        "        relevant_chunks.extend(page_chunks)\n",
        "\n",
        "    return relevant_chunks"
      ],
      "metadata": {
        "id": "9nPtf7R9x5XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demonstrate on a use case"
      ],
      "metadata": {
        "id": "nWcg3zLnx9aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the greenhouse effect?\"\n",
        "results = retrieve_hierarchical(query, summary_store, detailed_store)\n",
        "\n",
        "# Print results\n",
        "for chunk in results:\n",
        "    print(f\"Page: {chunk.metadata['page']}\")\n",
        "    print(f\"Content: {chunk.page_content}...\")  # Print first 100 characters\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Nmsl7AJx_xf",
        "outputId": "2b188310-b2e0-45cd-c03d-7cb9626830e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page: 0\n",
            "Content: driven by human activities, particularly the emission of greenhou se gases.  \n",
            "Chapter 2: Causes of Climate Change  \n",
            "Greenhouse Gases  \n",
            "The primary cause of recent climate change is the increase in greenhouse gases in the \n",
            "atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous \n",
            "oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is  essential \n",
            "for life on Earth, as it keeps the planet warm enough to support life. However, human \n",
            "activities have intensified this natural process, leading to a warmer climate.  \n",
            "Fossil Fuels  \n",
            "Burning fossil fuels for energy releases large amounts of CO2. This includes coal, oil, and \n",
            "natural gas used for electricity, heating, and transportation. The industrial revolution marked \n",
            "the beginning of a significant increase in fossil fuel consumption, which continues to rise \n",
            "today.  \n",
            "Coal...\n",
            "---\n",
            "Page: 0\n",
            "Content: Most of these climate changes are attributed to very small variations in Earth's orbit that \n",
            "change the amount of solar energy our planet receives. During the Holocene epoch, which \n",
            "began at the end of the last ice age, human societies f lourished, but the industrial era has seen \n",
            "unprecedented changes.  \n",
            "Modern Observations  \n",
            "Modern scientific observations indicate a rapid increase in global temperatures, sea levels, \n",
            "and extreme weather events. The Intergovernmental Panel on Climate Change (IPCC) has \n",
            "documented these changes extensively. Ice core samples, tree rings, and ocean sediments \n",
            "provide a historical record that scientists use to understand past climate conditions and \n",
            "predict future trends. The evidence overwhelmingly shows that recent changes are primarily \n",
            "driven by human activities, particularly the emission of greenhou se gases.  \n",
            "Chapter 2: Causes of Climate Change  \n",
            "Greenhouse Gases...\n",
            "---\n",
            "Page: 0\n",
            "Content: Understanding Climate Change  \n",
            "Chapter 1: Introduction to Climate Change  \n",
            "Climate change refers to significant, long -term changes in the global climate. The term \n",
            "\"global climate\" encompasses the planet's overall weather patterns, including temperature, \n",
            "precipitation, and wind patterns, over an extended period. Over the past cent ury, human \n",
            "activities, particularly the burning of fossil fuels and deforestation, have significantly \n",
            "contributed to climate change.  \n",
            "Historical Context  \n",
            "The Earth's climate has changed throughout history. Over the past 650,000 years, there have \n",
            "been seven cycles of glacial advance and retreat, with the abrupt end of the last ice age about \n",
            "11,700 years ago marking the beginning of the modern climate era and  human civilization. \n",
            "Most of these climate changes are attributed to very small variations in Earth's orbit that \n",
            "change the amount of solar energy our planet receives. During the Holocene epoch, which...\n",
            "---\n",
            "Page: 2\n",
            "Content: development of eco -friendly fertilizers and farming techniques is  essential for reducing the \n",
            "agricultural sector's carbon footprint.  \n",
            "Chapter 3: Effects of Climate Change  \n",
            "The effects of climate change are already being felt around the world and are projected to \n",
            "intensify in the coming decades. These effects include:  \n",
            "Rising Temperatures  \n",
            "Global temperatures have risen by about 1.2 degrees Celsius (2.2 degrees Fahrenheit) since \n",
            "the late 19th century. This warming is not uniform, with some regions experiencing more \n",
            "significant increases than others.  \n",
            "Heatwaves  \n",
            "Heatwaves are becoming more frequent and severe, posing risks to human health, agriculture, \n",
            "and infrastructure. Cities are particularly vulnerable due to the \"urban heat island\" effect. \n",
            "Heatwaves can lead to heat -related illnesses and exacerbate existing h ealth conditions.  \n",
            "Changing Seasons  \n",
            "Climate change is altering the timing and length of seasons, affecting ecosystems and human...\n",
            "---\n",
            "Page: 2\n",
            "Content: Ruminant animals, such as cows and sheep, produce methane during digestion. Manure \n",
            "management practices also contribute to methane and nitrous oxide emissions. Innovations in \n",
            "livestock feeding and waste management can help mitigate these emissions.  \n",
            "Rice Cultivation  \n",
            "Flooded rice paddies create anaerobic conditions that lead to methane production. Improved \n",
            "water management and rice varieties can help reduce these emissions. Research into \n",
            "sustainable rice farming practices is crucial for balancing food security and clim ate goals.  \n",
            "Fertilizers  \n",
            "The use of synthetic fertilizers in agriculture releases nitrous oxide, a potent greenhouse gas. \n",
            "Practices such as precision farming and organic fertilizers can mitigate these emissions. The \n",
            "development of eco -friendly fertilizers and farming techniques is  essential for reducing the \n",
            "agricultural sector's carbon footprint.  \n",
            "Chapter 3: Effects of Climate Change...\n",
            "---\n",
            "Page: 2\n",
            "Content: Changing Seasons  \n",
            "Climate change is altering the timing and length of seasons, affecting ecosystems and human \n",
            "activities. For example, spring is arriving earlier, and winters are becoming shorter and \n",
            "milder in many regions. This shift disrupts plant and animal life cycles a nd agricultural \n",
            "practices.  \n",
            "Melting Ice and Rising Sea Levels  \n",
            "Warmer temperatures are causing polar ice caps and glaciers to melt, contributing to rising \n",
            "sea levels. Sea levels have risen by about 20 centimeters (8 inches) in the past century, \n",
            "threatening coastal communities and ecosystems.  \n",
            "Polar Ice Melt...\n",
            "---\n",
            "Page: 1\n",
            "Content: Natural gas is the least carbon -intensive fossil fuel and is often seen as a \"bridge fuel\" to a \n",
            "lower -carbon future. However, its extraction and use still contribute to greenhouse gas \n",
            "emissions, particularly methane, which is a potent greenhouse gas. Innov ations in fracking \n",
            "technology have made natural gas more accessible, but this comes with environmental and \n",
            "health concerns.  \n",
            "Deforestation  \n",
            "Forests act as carbon sinks, absorbing CO2 from the atmosphere. When trees are cut down \n",
            "for timber or to clear land for agriculture, this stored carbon is released back into the \n",
            "atmosphere. Deforestation reduces the number of trees that can absorb CO2, exa cerbating the \n",
            "greenhouse effect.  \n",
            "Tropical Deforestation  \n",
            "Tropical rainforests are particularly important for carbon storage. Deforestation in the \n",
            "Amazon, Congo Basin, and Southeast Asia has significant impacts on global carbon cycles...\n",
            "---\n",
            "Page: 1\n",
            "Content: Tropical rainforests are particularly important for carbon storage. Deforestation in the \n",
            "Amazon, Congo Basin, and Southeast Asia has significant impacts on global carbon cycles \n",
            "and biodiversity. These regions are often cleared for agriculture, logging, and  mining, leading \n",
            "to habitat loss and species extinction.  \n",
            "Boreal Forests  \n",
            "Boreal forests, found in the northern regions of North America, Europe, and Asia, also play a \n",
            "crucial role in sequestering carbon. Logging and land -use changes in these regions contribute \n",
            "to climate change. These forests are vital for regulating the Earth' s climate and supporting \n",
            "indigenous communities and wildlife.  \n",
            "Agriculture  \n",
            "Agriculture contributes to climate change through methane emissions from livestock, rice \n",
            "paddies, and the use of synthetic fertilizers. Methane is a potent greenhouse gas with a much \n",
            "higher heat -trapping capability than CO2, albeit in smaller quantities.  \n",
            "Livestock Emissions...\n",
            "---\n"
          ]
        }
      ]
    }
  ]
}