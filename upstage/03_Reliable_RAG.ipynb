{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnXnqsGNdiLEGp1UD7p0rW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duper203/RAG_Techniques_with_upstage/blob/main/upstage/03_Reliable_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reliable-RAG üè∑Ô∏è\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "emWlJR1aOIWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Components\n",
        "1. Document Loading and Chunking:\n",
        "\n",
        "  Web-based documents are loaded and split into smaller, manageable chunks to facilitate efficient vector encoding and retrieval.\n",
        "\n",
        "  \n",
        "2. Vectorstore Creation:\n",
        "\n",
        "  Utilizes Chroma and Upstage embeddings to encode document chunks into a vector store, enabling efficient similarity-based retrieval.\n",
        "\n",
        "3. Document Relevancy Check:\n",
        "\n",
        "  Implements a relevance-checking mechanism using a language model to filter out non-relevant documents before answer generation.\n",
        "\n",
        "4. Answer Generation:\n",
        "\n",
        "  Employs a language model to generate concise answers based on the relevant documents retrieved.\n",
        "\n",
        "5. Hallucination Detection:\n",
        "\n",
        "  A dedicated hallucination detection step ensures that the generated answers are grounded in the retrieved documents, preventing the inclusion of unsupported or erroneous information.\n",
        "\n",
        "6. Document Snippet Highlighting:\n",
        "\n",
        "  The system identifies and highlights the specific document segments that were directly used to generate the answer, providing transparency and traceability.\n"
      ],
      "metadata": {
        "id": "8mnUs-G0OPoP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method Details\n",
        "* Document Relevancy Filtering:\n",
        "\n",
        "  By using a binary relevancy score generated by a language model, only the most relevant documents are passed on to the answer generation phase, reducing noise and improving the quality of the final answer.\n",
        "\n",
        "* Hallucination Check:\n",
        "\n",
        "  Before finalizing the answer, the system checks for hallucinations by verifying that the generated content is fully supported by the retrieved documents.\n",
        "\n",
        "* Snippet Highlighting:\n",
        "\n",
        "  This feature enhances transparency by showing the exact segments from the retrieved documents that contributed to the final answer.\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "hZLEJ-7NOrJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries and environment variables\n"
      ],
      "metadata": {
        "id": "rbxwK8wEPUB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install -qU langchain-upstage langchain-community langchain-Chroma"
      ],
      "metadata": {
        "id": "guaAiLcXGDb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bZp3g-glF4LP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"UPSTAGE_API_KEY\"] = userdata.get(\"UPSTAGE_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Vectorstore\n"
      ],
      "metadata": {
        "id": "g-ApMoR4Pbv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Build Index\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_upstage import UpstageEmbeddings\n",
        "\n",
        "# Set embeddings\n",
        "embedding_model = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
        "\n",
        "# Docs to index\n",
        "urls = [\n",
        "    \"https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io\",\n",
        "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/?ref=dl-staging-website.ghost.io\",\n",
        "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use/?ref=dl-staging-website.ghost.io\",\n",
        "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-4-planning/?ref=dl-staging-website.ghost.io\",\n",
        "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-5-multi-agent-collaboration/?ref=dl-staging-website.ghost.io\"\n",
        "]\n",
        "\n",
        "# Load\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=500, chunk_overlap=0\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "# Add to vectorstore\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag\",\n",
        "    embedding=embedding_model,\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever(\n",
        "                search_type=\"similarity\",\n",
        "                search_kwargs={'k': 4}, # number of documents to retrieve\n",
        "            )"
      ],
      "metadata": {
        "id": "3TqCpVx1GJ-V"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question"
      ],
      "metadata": {
        "id": "nBaCiTcRcePU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what are the differnt kind of agentic design patterns?\""
      ],
      "metadata": {
        "id": "F4u70N_AXlh7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve docs"
      ],
      "metadata": {
        "id": "IlWd_lRQXqDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = retriever.invoke(question)"
      ],
      "metadata": {
        "id": "d-FEHeUvQqHc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check what our doc looklike"
      ],
      "metadata": {
        "id": "uDDJZqZ3dAfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Title: {docs[0].metadata['title']}\\n\\nSource: {docs[0].metadata['source']}\\n\\nContent: {docs[0].page_content}\\n\")"
      ],
      "metadata": {
        "id": "rMJb0yzbHGdM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b10c3fa-2764-40a1-e6fd-00c39e6845ba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Agentic Design Patterns Part 5, Multi-Agent Collaboration\n",
            "\n",
            "Source: https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-5-multi-agent-collaboration/?ref=dl-staging-website.ghost.io\n",
            "\n",
            "Content: Agentic Design Patterns Part 5, Multi-Agent Collaboration‚ú® New course! Enroll in  Retrieval Optimization: From Tokenization to Vector QuantizationExplore CoursesAI NewsletterThe BatchAndrew's LetterData PointsML ResearchBlogCommunityForumEventsAmbassadorsAmbassador SpotlightResourcesCompanyAboutCareersContactStart LearningWeekly IssuesAndrew's LettersData PointsML ResearchBusinessScienceAI & SocietyCultureHardwareAI CareersAboutSubscribeThe BatchLettersArticleAgentic Design Patterns Part 5, Multi-Agent Collaboration Prompting an LLM to play different roles for different parts of a complex task summons a team of AI agents that can do the job more effectively.LettersTechnical InsightsPublishedApr 17, 2024Reading time3 min readShareDear friends,Multi-agent collaboration is the last of the four¬†key AI agentic design patterns¬†that I‚Äôve described in recent letters. Given a complex task like writing software, a multi-agent approach would break down the task into subtasks to be executed by different roles ‚Äî such as a software engineer, product manager, designer, QA (quality assurance) engineer, and so on ‚Äî and have different agents accomplish different subtasks.Different agents might be built by prompting one LLM (or, if you prefer, multiple LLMs) to carry out different tasks. For example, to build a software engineer agent, we might prompt the LLM: ‚ÄúYou are an expert in writing clear, efficient code. Write code to perform the task . . ..‚Äù¬†It might seem counterintuitive that, although we are making multiple calls to the same LLM, we apply the programming abstraction of using multiple agents. I‚Äôd like to offer a few reasons:It works! Many teams are getting good results with this method, and there‚Äôs nothing like results! Further, ablation studies (for example, in the AutoGen paper cited below) show that multiple agents give superior performance to a single agent.¬†Even though some LLMs today can accept very long input contexts (for instance, Gemini 1.5 Pro accepts 1 million tokens), their ability to truly understand long, complex inputs is mixed. An agentic workflow in which the LLM is prompted to focus on one thing at a time can give better performance. By telling it when it should play software engineer, we can also specify what is important in that role‚Äôs subtask. For example, the prompt above\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check document relevancy\n"
      ],
      "metadata": {
        "id": "TP-aKjGDRPmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_upstage import ChatUpstage\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatUpstage(model=\"solar-pro\")\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
        "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\n",
        "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
        "    Give a binary score 'yes' or 'no' to indicate whether the document is relevant to the question.\n",
        "    Respond with only 'yes' or 'no'.\"\"\"\n",
        "\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "retrieval_grader = grade_prompt | llm"
      ],
      "metadata": {
        "id": "rd8lfJJ1JfJo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter out the non-relevant docs"
      ],
      "metadata": {
        "id": "L3zhs7oZRWWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs_to_use = []\n",
        "for doc in docs:\n",
        "    print(doc.page_content, '\\n', '-'*50)\n",
        "    res = retrieval_grader.invoke({\"question\": question, \"document\": doc.page_content})\n",
        "    score = res.content.strip().lower()\n",
        "    print(f\"Relevance: {score}\\n\")\n",
        "    if score == 'yes':\n",
        "        docs_to_use.append(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZtN-F1EZOlU",
        "outputId": "b819b7f9-2b6c-4939-be56-a2c26f263313"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agentic Design Patterns Part 5, Multi-Agent Collaboration‚ú® New course! Enroll in  Retrieval Optimization: From Tokenization to Vector QuantizationExplore CoursesAI NewsletterThe BatchAndrew's LetterData PointsML ResearchBlogCommunityForumEventsAmbassadorsAmbassador SpotlightResourcesCompanyAboutCareersContactStart LearningWeekly IssuesAndrew's LettersData PointsML ResearchBusinessScienceAI & SocietyCultureHardwareAI CareersAboutSubscribeThe BatchLettersArticleAgentic Design Patterns Part 5, Multi-Agent Collaboration Prompting an LLM to play different roles for different parts of a complex task summons a team of AI agents that can do the job more effectively.LettersTechnical InsightsPublishedApr 17, 2024Reading time3 min readShareDear friends,Multi-agent collaboration is the last of the four¬†key AI agentic design patterns¬†that I‚Äôve described in recent letters. Given a complex task like writing software, a multi-agent approach would break down the task into subtasks to be executed by different roles ‚Äî such as a software engineer, product manager, designer, QA (quality assurance) engineer, and so on ‚Äî and have different agents accomplish different subtasks.Different agents might be built by prompting one LLM (or, if you prefer, multiple LLMs) to carry out different tasks. For example, to build a software engineer agent, we might prompt the LLM: ‚ÄúYou are an expert in writing clear, efficient code. Write code to perform the task . . ..‚Äù¬†It might seem counterintuitive that, although we are making multiple calls to the same LLM, we apply the programming abstraction of using multiple agents. I‚Äôd like to offer a few reasons:It works! Many teams are getting good results with this method, and there‚Äôs nothing like results! Further, ablation studies (for example, in the AutoGen paper cited below) show that multiple agents give superior performance to a single agent.¬†Even though some LLMs today can accept very long input contexts (for instance, Gemini 1.5 Pro accepts 1 million tokens), their ability to truly understand long, complex inputs is mixed. An agentic workflow in which the LLM is prompted to focus on one thing at a time can give better performance. By telling it when it should play software engineer, we can also specify what is important in that role‚Äôs subtask. For example, the prompt above \n",
            " --------------------------------------------------\n",
            "Relevance: yes\n",
            "\n",
            "Agentic Design Patterns Part 4: Planning‚ú® New course! Enroll in  Retrieval Optimization: From Tokenization to Vector QuantizationExplore CoursesAI NewsletterThe BatchAndrew's LetterData PointsML ResearchBlogCommunityForumEventsAmbassadorsAmbassador SpotlightResourcesCompanyAboutCareersContactStart LearningWeekly IssuesAndrew's LettersData PointsML ResearchBusinessScienceAI & SocietyCultureHardwareAI CareersAboutSubscribeThe BatchLettersArticleAgentic Design Patterns Part 4, Planning Large language models can drive powerful agents to execute complex tasks if you ask them to plan the steps before they act.LettersTechnical InsightsPublishedApr 10, 2024Reading time3 min readShareDear friends,Planning is a key¬†agentic AI design pattern¬†in which we use a large language model (LLM) to autonomously decide on what sequence of steps to execute to accomplish a larger task. For example, if we ask an agent to do online research on a given topic, we might use an LLM to break down the objective into smaller subtasks, such as researching specific subtopics, synthesizing findings, and compiling a report.¬†Many people had a ‚ÄúChatGPT moment‚Äù shortly after ChatGPT was released, when they played with it and were surprised that it significantly exceeded their expectation of what AI can do. If you have not yet had a similar ‚ÄúAI Agentic moment,‚Äù I hope you will soon. I had one several months ago, when I presented a live demo of a research agent I had implemented that had access to various online search tools.¬†I had tested this agent multiple times privately, during which it consistently used a web search tool to gather information and wrote up a summary. During the live demo, though, the web search API unexpectedly returned with a rate limiting error. I thought my demo was about to fail publicly, and I dreaded what was to come next. To my surprise, the agent pivoted deftly to a Wikipedia search tool ‚Äî which I had forgotten I‚Äôd given it ‚Äî and completed the task using Wikipedia instead of web search.¬†This was an AI Agentic moment of surprise for me. I think many people who haven‚Äôt experienced such a moment yet will do so in the coming months. It‚Äôs a beautiful thing when you see an agent autonomously decide to do things in ways that you had not anticipated, and succeed as a result!Many tasks \n",
            " --------------------------------------------------\n",
            "Relevance: yes\n",
            "\n",
            "al. (2023)‚ÄúUnderstanding the planning of LLM agents: A survey,‚Äù by Huang et al. (2024)Keep learning!AndrewRead \"Agentic Design Patterns Part 1: Four AI agent strategies that improve GPT-4 and GPT-3.5 performance\"Read \"Agentic Design Patterns Part 2: Reflection\"¬†Read \"Agentic Design Patterns Part 3: Tool Use\"Read \"Agentic Design Patterns Part 5: Multi-Agent Collaboration\"ShareSubscribe to The BatchStay updated with weekly AI News and Insights delivered to your inboxCoursesThe BatchCommunityCareersAbout \n",
            " --------------------------------------------------\n",
            "Relevance: yes\n",
            "\n",
            "from GPT-3.5 to GPT-4 is dwarfed by incorporating an iterative agent workflow. Indeed, wrapped in an agent loop, GPT-3.5 achieves up to 95.1%.¬†Open source agent tools and the academic literature on agents are proliferating, making this an exciting time but also a confusing one. To help put this work into perspective, I‚Äôd like to share a framework for categorizing design patterns for building agents. My team AI Fund is successfully using these patterns in many applications, and I hope you find them useful.Reflection: The LLM examines its own work to come up with ways to improve it.¬†Tool Use: The LLM is given tools such as web search, code execution, or any other function to help it gather information, take action, or process data.Planning: The LLM comes up with, and executes, a multistep plan to achieve a goal (for example, writing an outline for an essay, then doing online research, then writing a draft, and so on).Multi-agent collaboration: More than one AI agent work together, splitting up tasks and discussing and debating ideas, to come up with better solutions than a single agent would.Next week, I‚Äôll elaborate on these design patterns and offer suggested readings for each.Keep learning!AndrewRead \"Agentic Design Patterns Part 2: Reflection\"Read \"Agentic Design Patterns Part 3, Tool Use\"Read \"Agentic Design Patterns Part 4: Planning\"Read \"Agentic Design Patterns Part 5: Multi-Agent Collaboration\"ShareSubscribe to The BatchStay updated with weekly AI News and Insights delivered to your inboxCoursesThe BatchCommunityCareersAbout \n",
            " --------------------------------------------------\n",
            "Relevance: yes\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Result"
      ],
      "metadata": {
        "id": "fp7ESIhNevIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are an assistant for question-answering tasks. Answer the question based upon your knowledge.\n",
        "Use three-to-five sentences maximum and keep the answer concise.\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Retrieved documents: \\n\\n <docs>{documents}</docs> \\n\\n User question: <question>{question}</question>\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# LLM\n",
        "llm = ChatUpstage(model=\"solar-pro\")\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\".join(f\"<doc{i+1}>:\\nTitle:{doc.metadata['title']}\\nSource:{doc.metadata['source']}\\nContent:{doc.page_content}\\n</doc{i+1}>\\n\" for i, doc in enumerate(docs))\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run\n",
        "generation = rag_chain.invoke({\"documents\":format_docs(docs_to_use), \"question\": question})\n",
        "print(generation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jl62Djzqew31",
        "outputId": "f2dbf763-247b-4edd-b88a-c778039bf427"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The four types of agentic design patterns are: reflection, tool use, planning, and multi-agent collaboration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check for hallucinations"
      ],
      "metadata": {
        "id": "A9Cceg2Ue6NH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GC\n",
        "from langchain_upstage import UpstageGroundednessCheck\n",
        "\n",
        "groundedness_check = UpstageGroundednessCheck()\n",
        "\n",
        "gc_result = groundedness_check.invoke({\"context\": format_docs(docs_to_use), \"answer\": generation})\n",
        "\n",
        "print(gc_result)\n",
        "if gc_result.lower().startswith(\"grounded\"):\n",
        "    print(\"‚úÖ Groundedness check passed\")\n",
        "else:\n",
        "    print(\"‚ùå Groundedness check failed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isoeUZBQe7-H",
        "outputId": "d76833f1-3e89-4044-9aae-b7c03a2cf991"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grounded\n",
            "‚úÖ Groundedness check passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Highlight used docs"
      ],
      "metadata": {
        "id": "bwwb7gqRgfZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Data model\n",
        "class HighlightDocuments(BaseModel):\n",
        "    \"\"\"Return the specific part of a document used for answering the question.\"\"\"\n",
        "\n",
        "    id: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"List of id of docs used to answers the question\"\n",
        "    )\n",
        "\n",
        "    title: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"List of titles used to answers the question\"\n",
        "    )\n",
        "\n",
        "    source: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"List of sources used to answers the question\"\n",
        "    )\n",
        "\n",
        "    segment: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"List of direct segements from used documents that answers the question\"\n",
        "    )\n",
        "\n",
        "# LLM\n",
        "llm = ChatUpstage(model=\"solar-pro\")\n",
        "\n",
        "# parser\n",
        "parser = PydanticOutputParser(pydantic_object=HighlightDocuments)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are an advanced assistant for document search and retrieval. You are provided with the following:\n",
        "1. A question.\n",
        "2. A generated answer based on the question.\n",
        "3. A set of documents that were referenced in generating the answer.\n",
        "\n",
        "Your task is to identify and extract the exact inline segments from the provided documents that directly correspond to the content used to\n",
        "generate the given answer. The extracted segments must be verbatim snippets from the documents, ensuring a word-for-word match with the text\n",
        "in the provided documents.\n",
        "\n",
        "Ensure that:\n",
        "- (Important) Each segment is an exact match to a part of the document and is fully contained within the document text.\n",
        "- The relevance of each segment to the generated answer is clear and directly supports the answer provided.\n",
        "- (Important) If you didn't used the specific document don't mention it.\n",
        "\n",
        "Used documents: <docs>{documents}</docs> \\n\\n User question: <question>{question}</question> \\n\\n Generated answer: <answer>{generation}</answer>\n",
        "\n",
        "<format_instruction>\n",
        "{format_instructions}\n",
        "</format_instruction>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template= system,\n",
        "    input_variables=[\"documents\", \"question\", \"generation\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "# Chain\n",
        "doc_lookup = prompt | llm | parser\n",
        "\n",
        "# Run\n",
        "lookup_response = doc_lookup.invoke({\"documents\":format_docs(docs_to_use), \"question\": question, \"generation\": generation})"
      ],
      "metadata": {
        "id": "DQvkXGUqgjLx"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for id, title, source, segment in zip(lookup_response.id, lookup_response.title, lookup_response.source, lookup_response.segment):\n",
        "    print(f\"ID: {id}\\nTitle: {title}\\nSource: {source}\\nText Segment: {segment}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "filmpF5ygqB5",
        "outputId": "a2b72dad-83c3-4acb-f79a-5205a7a6343d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID: doc4\n",
            "Title: Four AI Agent Strategies That Improve GPT-4 and GPT-3.5 Performance\n",
            "Source: https://www.deeplearning.ai/the-batch/how-agentic-can-improve-llm-performance/?ref=dl-staging-website.ghost.io\n",
            "Text Segment: Four AI Agent Strigies That Improve GPT-4 and GPT-3.5 Performance\n",
            "\n"
          ]
        }
      ]
    }
  ]
}