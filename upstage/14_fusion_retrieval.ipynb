{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkdhe2DXXkISQUxck+SiIO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duper203/RAG_Techniques_with_upstage/blob/main/upstage/14_fusion_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fusion Retrieval in Document Search\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. PDF processing and text chunkin\n",
        "2. Vector store creation using FAISS and OpenAI embeddings\n",
        "3. BM25 index creation for keyword-based retrieval\n",
        "4. Custom fusion retrieval function that combines both methods\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Method Details\n",
        "\n",
        "1. Document Preprocessing\n",
        "\n",
        "\n",
        "2. Vector Store Creation\n",
        "\n",
        "\n",
        "3. BM25 Index Creation\n",
        "\n",
        "  1) A BM25 index is created from the same text chunks used for the vector store.\n",
        "  \n",
        "  2) This allows for keyword-based retrieval alongside the vector-based method.\n",
        "\n",
        "4. Fusion Retrieval Function\n",
        "\n",
        "The `fusion_retrieval` function is the core of this implementation:\n",
        "\n",
        "  * It takes a query and performs both vector-based and BM25-based retrieval.\n",
        "  * Scores from both methods are normalized to a common scale.\n",
        "  * A weighted combination of these scores is computed (controlled by the * alpha parameter).\n",
        "  Documents are ranked based on the combined scores, and the top-k results are returned.\n",
        "\n"
      ],
      "metadata": {
        "id": "_4vTJ_fSflby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries and set constants"
      ],
      "metadata": {
        "id": "VzM62JGiCKof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install -qU langchain-upstage langchain langchain-community faiss-cpu PyPDF2 rank_bm25"
      ],
      "metadata": {
        "id": "MFxiQBMlWmjl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_upstage import UpstageEmbeddings\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "from typing import List\n",
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as np\n",
        "\n",
        "os.environ[\"UPSTAGE_API_KEY\"] = userdata.get(\"UPSTAGE_API_KEY\")\n"
      ],
      "metadata": {
        "id": "z_2vs2-ob4R3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define document path\n"
      ],
      "metadata": {
        "id": "DhaAjrWHY20s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"data/Understanding_Climate_Change.pdf\""
      ],
      "metadata": {
        "id": "UhS1DAz7nkkZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encode the pdf to vector store and return split document from the step before to create BM25 instance"
      ],
      "metadata": {
        "id": "8ioLfSKZjZAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_pdf_and_get_split_documents(path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
        "\n",
        "    Args:\n",
        "        path: The path to the PDF file.\n",
        "        chunk_size: The desired size of each text chunk.\n",
        "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
        "\n",
        "    Returns:\n",
        "        A FAISS vector store containing the encoded book content.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load PDF documents\n",
        "    loader = PyPDFLoader(path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
        "    )\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    cleaned_texts = replace_t_with_space(texts)\n",
        "\n",
        "    # Create embeddings and vector store\n",
        "    embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
        "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
        "\n",
        "    return vectorstore, cleaned_texts"
      ],
      "metadata": {
        "id": "CGFRfHi1njbR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### functions\n",
        "\n",
        "* `replace_t_with_space`\n",
        "* `read_pdf_to_string`\n",
        "* `process_documents`"
      ],
      "metadata": {
        "id": "Tu9NzULOjckA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_t_with_space(list_of_documents):\n",
        "    \"\"\"\n",
        "    Replaces all tab characters ('\\t') with spaces in the page content of each document.\n",
        "\n",
        "    Args:\n",
        "        list_of_documents: A list of document objects, each with a 'page_content' attribute.\n",
        "\n",
        "    Returns:\n",
        "        The modified list of documents with tab characters replaced by spaces.\n",
        "    \"\"\"\n",
        "\n",
        "    for doc in list_of_documents:\n",
        "        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n",
        "    return list_of_documents"
      ],
      "metadata": {
        "id": "ubWXq1A7jb9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "def read_pdf_to_string(path):\n",
        "    \"\"\"\n",
        "    Read a PDF document from the specified path and return its content as a string.\n",
        "\n",
        "    Args:\n",
        "        path (str): The file path to the PDF document.\n",
        "\n",
        "    Returns:\n",
        "        str: The concatenated text content of all pages in the PDF document.\n",
        "\n",
        "    The function uses the 'fitz' library (PyMuPDF) to open the PDF document, iterate over each page,\n",
        "    extract the text content from each page, and append it to a single string.\n",
        "    \"\"\"\n",
        "    # Open the PDF document located at the specified path\n",
        "    doc = fitz.open(path)\n",
        "    content = \"\"\n",
        "    # Iterate over each page in the document\n",
        "    for page_num in range(len(doc)):\n",
        "        # Get the current page\n",
        "        page = doc[page_num]\n",
        "        # Extract the text content from the current page and append it to the content string\n",
        "        content += page.get_text()\n",
        "    return content\n"
      ],
      "metadata": {
        "id": "_TsoWhlndukk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_documents(documents):\n",
        "    \"\"\"\n",
        "    Processes a list of documents by splitting them into smaller chunks and creating a vector store.\n",
        "\n",
        "    Args:\n",
        "    - documents (list of str): A list of documents to be processed.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: A tuple containing:\n",
        "      - splits (list of str): The list of split document chunks.\n",
        "      - vector_store (FAISS): A FAISS vector store created from the split document chunks and their embeddings.\n",
        "    \"\"\"\n",
        "    embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    splits = text_splitter.split_documents(documents)\n",
        "    vector_store = FAISS.from_documents(splits, embeddings)\n",
        "    return splits, vector_store"
      ],
      "metadata": {
        "id": "WSdeiFf0ekkl"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create vectorstore and get the chunked documents"
      ],
      "metadata": {
        "id": "em2GJ0t7jUGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore, cleaned_texts = encode_pdf_and_get_split_documents(path)"
      ],
      "metadata": {
        "id": "JtSpybtVe4Sy"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a bm25 index for retrieving documents by keywords"
      ],
      "metadata": {
        "id": "P6vqIXRsjRgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bm25_index(documents: List[Document]) -> BM25Okapi:\n",
        "    \"\"\"\n",
        "    Create a BM25 index from the given documents.\n",
        "\n",
        "    BM25 (Best Matching 25) is a ranking function used in information retrieval.\n",
        "    It's based on the probabilistic retrieval framework and is an improvement over TF-IDF.\n",
        "\n",
        "    Args:\n",
        "    documents (List[Document]): List of documents to index.\n",
        "\n",
        "    Returns:\n",
        "    BM25Okapi: An index that can be used for BM25 scoring.\n",
        "    \"\"\"\n",
        "    # Tokenize each document by splitting on whitespace\n",
        "    # This is a simple approach and could be improved with more sophisticated tokenization\n",
        "    tokenized_docs = [doc.page_content.split() for doc in documents]\n",
        "    return BM25Okapi(tokenized_docs)"
      ],
      "metadata": {
        "id": "ST7KlgAFfZGd"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bm25 = create_bm25_index(cleaned_texts) # Create BM25 index from the cleaned texts (chunks)\n"
      ],
      "metadata": {
        "id": "uXeushXFfZ_l"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a function that retrieves both semantically and by keyword, normalizes the scores and gets the top k documents"
      ],
      "metadata": {
        "id": "Xbmd2rMFjPM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fusion_retrieval(vectorstore, bm25, query: str, k: int = 5, alpha: float = 0.5) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Perform fusion retrieval combining keyword-based (BM25) and vector-based search.\n",
        "\n",
        "    Args:\n",
        "    vectorstore (VectorStore): The vectorstore containing the documents.\n",
        "    bm25 (BM25Okapi): Pre-computed BM25 index.\n",
        "    query (str): The query string.\n",
        "    k (int): The number of documents to retrieve.\n",
        "    alpha (float): The weight for vector search scores (1-alpha will be the weight for BM25 scores).\n",
        "\n",
        "    Returns:\n",
        "    List[Document]: The top k documents based on the combined scores.\n",
        "    \"\"\"\n",
        "    # Step 1: Get all documents from the vectorstore\n",
        "    all_docs = vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal)\n",
        "\n",
        "    # Step 2: Perform BM25 search\n",
        "    bm25_scores = bm25.get_scores(query.split())\n",
        "\n",
        "    # Step 3: Perform vector search\n",
        "    vector_results = vectorstore.similarity_search_with_score(query, k=len(all_docs))\n",
        "\n",
        "    # Step 4: Normalize scores\n",
        "    vector_scores = np.array([score for _, score in vector_results])\n",
        "    vector_scores = 1 - (vector_scores - np.min(vector_scores)) / (np.max(vector_scores) - np.min(vector_scores))\n",
        "\n",
        "    bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores))\n",
        "\n",
        "    # Step 5: Combine scores\n",
        "    combined_scores = alpha * vector_scores + (1 - alpha) * bm25_scores\n",
        "\n",
        "    # Step 6: Rank documents\n",
        "    sorted_indices = np.argsort(combined_scores)[::-1]\n",
        "\n",
        "    # Step 7: Return top k documents\n",
        "    return [all_docs[i] for i in sorted_indices[:k]]"
      ],
      "metadata": {
        "id": "ZL1XDTpYfbht"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Case example"
      ],
      "metadata": {
        "id": "RgIZyv9ejMkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query\n",
        "query = \"What are the impacts of climate change on the environment?\"\n",
        "\n",
        "# Perform fusion retrieval\n",
        "top_docs = fusion_retrieval(vectorstore, bm25, query, k=5, alpha=0.5)\n",
        "docs_content = [doc.page_content for doc in top_docs]\n",
        "show_context(docs_content) # from helpers_function.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKLnD0TFfcmR",
        "outputId": "a2793a32-942c-4e77-bf7f-ea32e06908c0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context 1:\n",
            "water as a byproduct. Fuel cell vehicles (FCVs) offer a clean alternative to conventional \n",
            "vehicles, particularly for heavy -duty applications like trucks and buses. D eveloping a robust \n",
            "hydrogen infrastructure is essential for their success.  \n",
            "Public Transportation Innovations  \n",
            "Investments in efficient and reliable public transportation systems can reduce the number of \n",
            "private vehicles on the road, lowering emissions. Innovations include electric buses, light rail \n",
            "systems, and bike -sharing programs. Urban planning that prioritize s public transportation and \n",
            "non-motorized transit is key.  \n",
            "Sustainable Agriculture and Land Use  \n",
            "Precision Agriculture  \n",
            "Precision agriculture uses technology to monitor and manage crop production more \n",
            "effectively. Techniques include GPS -guided equipment, soil sensors, and data analytics. \n",
            "These methods can optimize resource use, reduce emissions, and increase yields.  \n",
            "Agroforestry\n",
            "\n",
            "\n",
            "Context 2:\n",
            "negative emissions. The captured CO2 can be stored or used in various applications. Scaling \n",
            "DAC technology and reducing costs are critical for its widespread adoption.  \n",
            "Chapter 16: Global Cooperation and Governance  \n",
            "International Agreements  \n",
            "Paris Agreement  \n",
            "The Paris Agreement is a landmark international accord that aims to limit global warming to \n",
            "well below 2 degrees Celsius above pre -industrial levels, with efforts to limit the increase to \n",
            "1.5 degrees Celsius. Countries submit nationally determined contribu tions (NDCs) outlining \n",
            "their climate action plans. Regular reviews and updates of NDCs are essential for meeting the \n",
            "agreement's goals.\n",
            "\n",
            "\n",
            "Context 3:\n",
            "Pumped hydro storage involves storing energy by moving water between two reservoirs at \n",
            "different elevations. During periods of high electricity demand, water is released from the \n",
            "upper reservoir to generate electricity. This method provides a large -scale, long-duration \n",
            "storage solution.  \n",
            "Carbon Capture and Utilization  \n",
            "Direct Air Capture  \n",
            "Direct air capture (DAC) technology involves removing CO2 directly from the atmosphere. \n",
            "The captured CO2 can be stored underground or used in various industrial processes. DAC is \n",
            "an emerging technology with the potential to play a significant role in achie ving net -zero \n",
            "emissions.  \n",
            "Carbon Utilization\n",
            "\n",
            "\n",
            "Context 4:\n",
            "Kyoto Protocol  \n",
            "The Kyoto Protocol, adopted in 1997, set binding emission reduction targets for developed \n",
            "countries. It was the first major international treaty to address climate change. The protocol \n",
            "laid the groundwork for subsequent agreements, highlighting the importa nce of collective \n",
            "action.  \n",
            "Montreal Protocol  \n",
            "The Montreal Protocol, designed to protect the ozone layer by phasing out ozone -depleting \n",
            "substances, has also contributed to climate mitigation. The Kigali Amendment to the protocol \n",
            "targets hydrofluorocarbons (HFCs), potent greenhouse gases, demonstrating  the treaty's \n",
            "evolving role in climate protection.  \n",
            "Regional and National Initiatives  \n",
            "European Green Deal  \n",
            "The European Green Deal is an ambitious plan to make Europe the first climate -neutral \n",
            "continent by 2050. It includes measures to reduce emissions, promote clean energy, and \n",
            "support sustainable agriculture and biodiversity. The deal also aims to create jobs  and \n",
            "enhance economic resilience.\n",
            "\n",
            "\n",
            "Context 5:\n",
            "and storage, and sustainable agriculture. Collaboration between governments , industries, and \n",
            "academia is essential for fostering innovation.  \n",
            "Renewable Energy Technology  \n",
            "Investing in research and development of renewable energy technologies can lead to more \n",
            "efficient and cost -effective solutions. Emerging technologies, such as advanced solar cells \n",
            "and wind turbine designs, hold promise for the future.  \n",
            "Carbon Capture and Storage  \n",
            "Carbon capture and storage (CCS) technologies aim to capture CO2 emissions from industrial \n",
            "sources and store them underground. These technologies are critical for reducing emissions \n",
            "from hard -to-abate sectors and achieving net -zero targets.  \n",
            "Sustainable Agriculture  \n",
            "Innovations in sustainable agriculture can help reduce emissions, enhance food security, and \n",
            "protect ecosystems. Practices such as agroforestry, precision farming, and regenerative \n",
            "agriculture offer pathways to a more sustainable and resilient food system.\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}