{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8ovbQHOe7ZtKMiPDK9jac",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duper203/RAG_Techniques_with_upstage/blob/main/upstage/21__adaptive_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptive Retrieval-Augmented Generation (RAG) System\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. Query Classifier: Determines the type of query (Factual, Analytical, Opinion, or Contextual).\n",
        "\n",
        "2. Adaptive Retrieval Strategies: Four distinct strategies tailored to different query types:\n",
        "\n",
        "  * Factual Strategy\n",
        "  * Analytical Strategy\n",
        "  * Opinion Strategy\n",
        "  * Contextual Strategy\n",
        "3. LLM Integration: LLMs are used throughout the process to enhance retrieval and ranking.\n",
        "\n",
        "4. OpenAI GPT Model: Generates the final response using the retrieved documents as context.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Method Details\n",
        "\n",
        "1. Query Classification\n",
        "2. Adaptive Retrieval Strategies\n",
        "3. LLM-Enhanced Ranking\n",
        "4. Response Generation"
      ],
      "metadata": {
        "id": "_4vTJ_fSflby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install -qU langchain-upstage langchain langchain-community faiss-cpu PyMuPDF"
      ],
      "metadata": {
        "id": "MFxiQBMlWmjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d704689-77c8-4078-a89e-a915daaf9edd"
      },
      "execution_count": 1,
      
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import asyncio\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain_upstage import ChatUpstage, UpstageEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from typing import Dict, Any\n",
        "from langchain.docstore.document import Document\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "\n",
        "\n",
        "os.environ[\"UPSTAGE_API_KEY\"] = userdata.get(\"UPSTAGE_API_KEY\")"
      ],
      "metadata": {
        "id": "z_2vs2-ob4R3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "884fa8d2-584d-498d-915c-60e082a1bd7b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class categories_options(BaseModel):\n",
        "        category: str = Field(description=\"The category of the query, the options are: Factual, Analytical, Opinion, or Contextual\", example=\"Factual\")\n",
        "\n",
        "\n",
        "class QueryClassifier:\n",
        "    def __init__(self):\n",
        "        self.llm = ChatUpstage()\n",
        "        self.prompt = PromptTemplate(\n",
        "            input_variables=[\"query\"],\n",
        "            template=\"Classify the following query into one of these categories: Factual, Analytical, Opinion, or Contextual.\\nQuery: {query}\\nCategory:\"\n",
        "        )\n",
        "        self.chain = self.prompt | self.llm.with_structured_output(categories_options)\n",
        "\n",
        "\n",
        "    def classify(self, query):\n",
        "        print(\"clasiffying query\")\n",
        "        return self.chain.invoke(query).category"
      ],
      "metadata": {
        "id": "kkYsUB5gS8wh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseRetrievalStrategy:\n",
        "    def __init__(self, texts):\n",
        "        self.embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
        "        text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=0)\n",
        "        self.documents = text_splitter.create_documents(texts)\n",
        "        self.db = FAISS.from_documents(self.documents, self.embeddings)\n",
        "        self.llm = ChatUpstage()\n",
        "\n",
        "\n",
        "    def retrieve(self, query, k=4):\n",
        "        return self.db.similarity_search(query, k=k)"
      ],
      "metadata": {
        "id": "F2Km7LonS-D4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class relevant_score(BaseModel):\n",
        "        score: float = Field(description=\"The relevance score of the document to the query\", example=8.0)\n",
        "\n",
        "class FactualRetrievalStrategy(BaseRetrievalStrategy):\n",
        "    def retrieve(self, query, k=4):\n",
        "        print(\"retrieving factual\")\n",
        "        # Use LLM to enhance the query\n",
        "        enhanced_query_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\"],\n",
        "            template=\"Enhance this factual query for better information retrieval: {query}\"\n",
        "        )\n",
        "        query_chain = enhanced_query_prompt | self.llm\n",
        "        enhanced_query = query_chain.invoke(query).content\n",
        "        print(f'enhande query: {enhanced_query}')\n",
        "\n",
        "        # Retrieve documents using the enhanced query\n",
        "        docs = self.db.similarity_search(enhanced_query, k=k*2)\n",
        "\n",
        "        # Use LLM to rank the relevance of retrieved documents\n",
        "        ranking_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"doc\"],\n",
        "            template=\"On a scale of 1-10, how relevant is this document to the query: '{query}'?\\nDocument: {doc}\\nRelevance score:\"\n",
        "        )\n",
        "        ranking_chain = ranking_prompt | self.llm.with_structured_output(relevant_score)\n",
        "\n",
        "        ranked_docs = []\n",
        "        print(\"ranking docs\")\n",
        "        for doc in docs:\n",
        "            input_data = {\"query\": enhanced_query, \"doc\": doc.page_content}\n",
        "            score = float(ranking_chain.invoke(input_data).score)\n",
        "            ranked_docs.append((doc, score))\n",
        "\n",
        "        # Sort by relevance score and return top k\n",
        "        ranked_docs.sort(key=lambda x: x[1], reverse=True)\n",
        "        return [doc for doc, _ in ranked_docs[:k]]"
      ],
      "metadata": {
        "id": "eRy_8QcATArY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelectedIndices(BaseModel):\n",
        "    indices: List[int] = Field(description=\"Indices of selected documents\", example=[0, 1, 2, 3])\n",
        "\n",
        "class SubQueries(BaseModel):\n",
        "    sub_queries: List[str] = Field(description=\"List of sub-queries for comprehensive analysis\", example=[\"What is the population of New York?\", \"What is the GDP of New York?\"])\n",
        "\n",
        "class AnalyticalRetrievalStrategy(BaseRetrievalStrategy):\n",
        "    def retrieve(self, query, k=4):\n",
        "        print(\"retrieving analytical\")\n",
        "        # Use LLM to generate sub-queries for comprehensive analysis\n",
        "        sub_queries_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"k\"],\n",
        "            template=\"Generate {k} sub-questions for: {query}\"\n",
        "        )\n",
        "\n",
        "        llm = ChatUpstage(model='solar-pro')\n",
        "        sub_queries_chain = sub_queries_prompt | llm.with_structured_output(SubQueries)\n",
        "\n",
        "        input_data = {\"query\": query, \"k\": k}\n",
        "        sub_queries = sub_queries_chain.invoke(input_data).sub_queries\n",
        "        print(f'sub queries for comprehensive analysis: {sub_queries}')\n",
        "\n",
        "        all_docs = []\n",
        "        for sub_query in sub_queries:\n",
        "            all_docs.extend(self.db.similarity_search(sub_query, k=2))\n",
        "\n",
        "        # Use LLM to ensure diversity and relevance\n",
        "        diversity_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"docs\", \"k\"],\n",
        "            template=\"\"\"Select the most diverse and relevant set of {k} documents for the query: '{query}'\\nDocuments: {docs}\\n\n",
        "            Return only the indices of selected documents as a list of integers.\"\"\"\n",
        "        )\n",
        "        diversity_chain = diversity_prompt | self.llm.with_structured_output(SelectedIndices)\n",
        "        docs_text = \"\\n\".join([f\"{i}: {doc.page_content[:50]}...\" for i, doc in enumerate(all_docs)])\n",
        "        input_data = {\"query\": query, \"docs\": docs_text, \"k\": k}\n",
        "        selected_indices_result = diversity_chain.invoke(input_data).indices\n",
        "        print(f'selected diverse and relevant documents')\n",
        "\n",
        "        return [all_docs[i] for i in selected_indices_result if i < len(all_docs)]\n"
      ],
      "metadata": {
        "id": "D6AYmBA9TKtA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OpinionRetrievalStrategy(BaseRetrievalStrategy):\n",
        "    def retrieve(self, query, k=3):\n",
        "        print(\"retrieving opinion\")\n",
        "        # Use LLM to identify potential viewpoints\n",
        "        viewpoints_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"k\"],\n",
        "            template=\"Identify {k} distinct viewpoints or perspectives on the topic: {query}\"\n",
        "        )\n",
        "        viewpoints_chain = viewpoints_prompt | self.llm\n",
        "        input_data = {\"query\": query, \"k\": k}\n",
        "        viewpoints = viewpoints_chain.invoke(input_data).content.split('\\n')\n",
        "        print(f'viewpoints: {viewpoints}')\n",
        "\n",
        "        all_docs = []\n",
        "        for viewpoint in viewpoints:\n",
        "            all_docs.extend(self.db.similarity_search(f\"{query} {viewpoint}\", k=2))\n",
        "\n",
        "        # Use LLM to classify and select diverse opinions\n",
        "        opinion_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"docs\", \"k\"],\n",
        "            template=\"Classify these documents into distinct opinions on '{query}' and select the {k} most representative and diverse viewpoints:\\nDocuments: {docs}\\nSelected indices:\"\n",
        "        )\n",
        "        opinion_chain = opinion_prompt | self.llm.with_structured_output(SelectedIndices)\n",
        "\n",
        "        docs_text = \"\\n\".join([f\"{i}: {doc.page_content[:100]}...\" for i, doc in enumerate(all_docs)])\n",
        "        input_data = {\"query\": query, \"docs\": docs_text, \"k\": k}\n",
        "        selected_indices = opinion_chain.invoke(input_data).indices\n",
        "        print(f'selected diverse and relevant documents')\n",
        "\n",
        "        return [all_docs[int(i)] for i in selected_indices.split() if i.isdigit() and int(i) < len(all_docs)]"
      ],
      "metadata": {
        "id": "ApVOUzmzThWE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ContextualRetrievalStrategy(BaseRetrievalStrategy):\n",
        "    def retrieve(self, query, k=4, user_context=None):\n",
        "        print(\"retrieving contextual\")\n",
        "        # Use LLM to incorporate user context into the query\n",
        "        context_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"context\"],\n",
        "            template=\"Given the user context: {context}\\nReformulate the query to best address the user's needs: {query}\"\n",
        "        )\n",
        "        context_chain = context_prompt | self.llm\n",
        "        input_data = {\"query\": query, \"context\": user_context or \"No specific context provided\"}\n",
        "        contextualized_query = context_chain.invoke(input_data).content\n",
        "        print(f'contextualized query: {contextualized_query}')\n",
        "\n",
        "        # Retrieve documents using the contextualized query\n",
        "        docs = self.db.similarity_search(contextualized_query, k=k*2)\n",
        "\n",
        "        # Use LLM to rank the relevance of retrieved documents considering the user context\n",
        "        ranking_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"context\", \"doc\"],\n",
        "            template=\"Given the query: '{query}' and user context: '{context}', rate the relevance of this document on a scale of 1-10:\\nDocument: {doc}\\nRelevance score:\"\n",
        "        )\n",
        "        ranking_chain = ranking_prompt | self.llm.with_structured_output(relevant_score)\n",
        "        print(\"ranking docs\")\n",
        "\n",
        "        ranked_docs = []\n",
        "        for doc in docs:\n",
        "            input_data = {\"query\": contextualized_query, \"context\": user_context or \"No specific context provided\", \"doc\": doc.page_content}\n",
        "            score = float(ranking_chain.invoke(input_data).score)\n",
        "            ranked_docs.append((doc, score))\n",
        "\n",
        "\n",
        "        # Sort by relevance score and return top k\n",
        "        ranked_docs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return [doc for doc, _ in ranked_docs[:k]]"
      ],
      "metadata": {
        "id": "sgi_o7N0TjLM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaptiveRetriever:\n",
        "    def __init__(self, texts: List[str]):\n",
        "        self.classifier = QueryClassifier()\n",
        "        self.strategies = {\n",
        "            \"Factual\": FactualRetrievalStrategy(texts),\n",
        "            \"Analytical\": AnalyticalRetrievalStrategy(texts),\n",
        "            \"Opinion\": OpinionRetrievalStrategy(texts),\n",
        "            \"Contextual\": ContextualRetrievalStrategy(texts)\n",
        "        }\n",
        "\n",
        "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
        "        category = self.classifier.classify(query)\n",
        "        strategy = self.strategies[category]\n",
        "        return strategy.retrieve(query)"
      ],
      "metadata": {
        "id": "l0Y-R-OaTk8T"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PydanticAdaptiveRetriever(BaseRetriever):\n",
        "    adaptive_retriever: AdaptiveRetriever = Field(exclude=True)\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
        "        return self.adaptive_retriever.get_relevant_documents(query)\n",
        "\n",
        "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
        "        return self.get_relevant_documents(query)"
      ],
      "metadata": {
        "id": "jeTABraNTmf-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaptiveRAG:\n",
        "    def __init__(self, texts: List[str]):\n",
        "        adaptive_retriever = AdaptiveRetriever(texts)\n",
        "        self.retriever = PydanticAdaptiveRetriever(adaptive_retriever=adaptive_retriever)\n",
        "        self.llm = ChatUpstage(model='solar-pro')\n",
        "\n",
        "        # Create a custom prompt\n",
        "        prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "        {context}\n",
        "\n",
        "        Question: {question}\n",
        "        Answer:\"\"\"\n",
        "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "        # Create the LLM chain\n",
        "        self.llm_chain = prompt | self.llm\n",
        "\n",
        "\n",
        "\n",
        "    def answer(self, query: str) -> str:\n",
        "        docs = self.retriever.get_relevant_documents(query)\n",
        "        input_data = {\"context\": \"\\n\".join([doc.page_content for doc in docs]), \"question\": query}\n",
        "        return self.llm_chain.invoke(input_data)"
      ],
      "metadata": {
        "id": "VVKjHoTSTo5u"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage\n",
        "texts = [\n",
        "    \"The Earth is the third planet from the Sun and the only astronomical object known to harbor life.\"\n",
        "    ]\n",
        "rag_system = AdaptiveRAG(texts)"
      ],
      "metadata": {
        "id": "j9Mvb8NETqfN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "factual_result = rag_system.answer(\"What is the distance between the Earth and the Sun?\")\n",
        "print(f\"Answer: {factual_result}\")\n",
        "\n",
        "analytical_result = rag_system.answer(\"How does the Earth's distance from the Sun affect its climate?\").content\n",
        "print(f\"Answer: {analytical_result}\")\n",
        "\n",
        "opinion_result = rag_system.answer(\"What are the different theories about the origin of life on Earth?\").content\n",
        "print(f\"Answer: {opinion_result}\")\n",
        "\n",
        "contextual_result = rag_system.answer(\"How does the Earth's position in the Solar System influence its habitability?\").content\n",
        "print(f\"Answer: {contextual_result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na5Sr-PoTs7D",
        "outputId": "0b91e5a4-10ed-4f56-a2f2-66bac710e71b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-61b8de62969f>:23: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = self.retriever.get_relevant_documents(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clasiffying query\n",
            "retrieving factual\n",
            "enhande query: \"What is the average distance between the Earth and the Sun, and how does this distance vary throughout the year?\"\n",
            "ranking docs\n",
            "Answer: content='The average distance between the Earth and the Sun is about 93 million miles or 150 million kilometers.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 98, 'total_tokens': 124, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'solar-pro-preview-240910', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-ccb51cf9-299d-418e-832e-19eb90a78835-0' usage_metadata={'input_tokens': 98, 'output_tokens': 26, 'total_tokens': 124, 'input_token_details': {}, 'output_token_details': {}}\n",
            "clasiffying query\n",
            "retrieving factual\n",
            "enhande query: To enhance the factual query for better information retrieval, you can consider adding specific keywords related to the Earth's distance from the Sun and its impact on climate. Here's a revised version of the query:\n",
            "\n",
            "\"How does the Earth's orbital distance from the Sun, including factors like eccentricity and axial tilt, influence its climate and weather patterns?\"\n",
            "\n",
            "This revised query includes specific keywords such as \"orbital distance,\" \"eccentricity,\" \"axial tilt,\" \"climate,\" and \"weather patterns.\" These keywords will help you find more relevant and detailed information about the Earth's distance from the Sun and its impact on climate.\n",
            "ranking docs\n",
            "Answer: The Earth's distance from the Sun plays a significant role in its climate. The Earth is located in the \"Goldilocks zone\" or \"habitable zone,\" which is the perfect distance from the Sun that allows for liquid water to exist on the planet's surface. This distance, combined with the Earth's atmospheric composition, helps maintain a climate that supports life. If the Earth were much closer to the Sun, it would be too hot, and if it were much farther away, it would be too cold.\n",
            "clasiffying query\n",
            "retrieving factual\n",
            "enhande query: To enhance the factual query for better information retrieval, you can try the following:\n",
            "\n",
            "1. Be more specific: Instead of asking about the \"origin of life,\" specify what aspect of the origin you're interested in, such as the \"formation of the first living cell\" or \"the emergence of life from non-life.\"\n",
            "\n",
            "2. Use keywords: Incorporate keywords related to the theories you're interested in, such as \"abiogenesis,\" \"panspermia,\" \"primordial soup,\" or \"RNA world.\"\n",
            "\n",
            "3. Specify the type of information you're looking for: Are you interested in scientific theories, philosophical perspectives, or both? Make this clear in your query.\n",
            "\n",
            "4. Use search operators: If you're using a search engine, take advantage of search operators like \"AND\" and \"OR\" to narrow down your results. For example, you could search for \"abiogenesis AND RNA world\" to find information about the RNA world theory of abiogenesis.\n",
            "\n",
            "5. Use quotation marks: If you're looking for a specific phrase or title, use quotation marks around it. For example, \"theory of abiogenesis\" will return results with that exact phrase.\n",
            "\n",
            "6. Use a search engine that specializes in academic resources: If you're looking for scholarly articles or research papers, use a search engine like Google Scholar or a database like JSTOR.\n",
            "\n",
            "Here's an example of an enhanced query:\n",
            "\n",
            "\"Theories of the formation of the first living cell\" OR \"emergence of life from non-life\" AND \"scientific theories\" AND \"RNA world\" AND \"abiogenesis\" -\"philosophical perspectives\"\n",
            "ranking docs\n",
            "Answer: There are several theories about the origin of life on Earth. Some of the most well-known ones include the primordial soup theory, which suggests that life originated in a warm, watery environment rich in organic molecules. Another theory is the hydrothermal vent hypothesis, which proposes that life began near underwater volcanic vents. There's also the panspermia theory, which suggests that life may have originated elsewhere in the universe and was brought to Earth through meteorites or comets. Lastly, there's the RNA world hypothesis, which proposes that life started with RNA molecules that could replicate themselves.\n",
            "clasiffying query\n",
            "retrieving factual\n",
            "enhande query: To enhance the factual query for better information retrieval, we can add more specific details and break down the query into smaller, more focused questions. Here's an enhanced version of the query:\n",
            "\n",
            "How does the Earth's position in the Solar System influence its distance from the Sun?\n",
            "How does the Earth's distance from the Sun affect its temperature and climate?\n",
            "How does the Earth's position in the Solar System influence its axial tilt and seasons?\n",
            "How does the Earth's axial tilt and seasons affect its habitability?\n",
            "How does the Earth's position in the Solar System influence its orbit and gravitational interactions with other celestial bodies?\n",
            "How do the Earth's orbit and gravitational interactions with other celestial bodies affect its habitability?\n",
            "\n",
            "By breaking down the query into smaller, more focused questions, it becomes easier to retrieve specific and relevant information on each aspect of the Earth's position in the Solar System and its influence on habitability.\n",
            "ranking docs\n",
            "Answer: The Earth's position in the Solar System significantly influences its habitability. It is located in the \"Goldilocks Zone\" or the habitable zone, which is the region around a star where conditions are just right for liquid water to exist on a planet's surface. Being the third planet from the Sun, Earth receives an optimal amount of solar energy, which maintains a temperature range suitable for life as we know it.\n"
          ]
        }
      ]
    }
  ]
}
