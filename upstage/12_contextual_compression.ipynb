{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNX6i/IFgMxYF4bdtZG8ueD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duper203/RAG_Techniques_with_upstage/blob/main/upstage/12_contextual_compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Contextual Compression in Document Retrieval\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. Vector store creation from a PDF document\n",
        "2. Base retriever setup\n",
        "3. LLM-based contextual compressor\n",
        "4. Contextual compression retriever\n",
        "5. Question-answering chain integrating the compressed retriever\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Method Details\n",
        "\n",
        "1. Document Preprocessing and Vector Store Creation\n",
        "\n",
        "  The PDF is processed and encoded into a vector store using a custom `encode_pdf` function.\n",
        "\n",
        "\n",
        "2. Retriever and Compressor Setup\n",
        "\n",
        "  1) A base retriever is created from the vector store.\n",
        "\n",
        "  2) An LLM-based contextual compressor (LLMChainExtractor) is initialized using Upstage Solar\n",
        "\n",
        "3. Contextual Compression Retriever\n",
        "  \n",
        "  1) The base retriever and compressor are combined into a ContextualCompressionRetriever.\n",
        "\n",
        "  2) This retriever first fetches documents using the base retriever, then applies the compressor to extract the most relevant information.\n",
        "\n",
        "4. Question-Answering Chain\n",
        "\n",
        "  1) A RetrievalQA chain is created, integrating the compression retriever.\n",
        "\n",
        "  2) This chain uses the compressed and extracted information to generate answers to queries.\n"
      ],
      "metadata": {
        "id": "_4vTJ_fSflby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install -qU langchain-upstage langchain langchain-community faiss-cpu"
      ],
      "metadata": {
        "id": "MFxiQBMlWmjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e752b8cf-7386-41c4-bd94-450200a4edc8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_upstage import UpstageEmbeddings\n",
        "\n",
        "os.environ[\"UPSTAGE_API_KEY\"] = userdata.get(\"UPSTAGE_API_KEY\")"
      ],
      "metadata": {
        "id": "z_2vs2-ob4R3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define document(s) path & Read PDf to string"
      ],
      "metadata": {
        "id": "DhaAjrWHY20s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"data/Understanding_Climate_Change.pdf\""
      ],
      "metadata": {
        "id": "fSlBtO2-UuAq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a vector store"
      ],
      "metadata": {
        "id": "7eA-1cf2j9y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "def encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
        "\n",
        "    Args:\n",
        "        path: The path to the PDF file.\n",
        "        chunk_size: The desired size of each text chunk.\n",
        "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
        "\n",
        "    Returns:\n",
        "        A FAISS vector store containing the encoded book content.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load PDF documents\n",
        "    loader = PyPDFLoader(path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
        "    )\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    cleaned_texts = replace_t_with_space(texts)\n",
        "\n",
        "    # Create embeddings and vector store\n",
        "    embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
        "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
        "\n",
        "    return vectorstore\n",
        "def replace_t_with_space(list_of_documents):\n",
        "    \"\"\"\n",
        "    Replaces all tab characters ('\\t') with spaces in the page content of each document.\n",
        "\n",
        "    Args:\n",
        "        list_of_documents: A list of document objects, each with a 'page_content' attribute.\n",
        "\n",
        "    Returns:\n",
        "        The modified list of documents with tab characters replaced by spaces.\n",
        "    \"\"\"\n",
        "\n",
        "    for doc in list_of_documents:\n",
        "        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n",
        "    return list_of_documents"
      ],
      "metadata": {
        "id": "fIjUgPG9hsg-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = encode_pdf(path)"
      ],
      "metadata": {
        "id": "_xVuZ-diU6r2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a retriever + contexual compressor + combine them\n"
      ],
      "metadata": {
        "id": "N7WQBCcUgLL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_upstage import ChatUpstage\n",
        "\n",
        "# Create a retriever\n",
        "retriever = vector_store.as_retriever()\n",
        "\n",
        "\n",
        "#Create a contextual compressor\n",
        "llm = ChatUpstage(model=\"solar-pro\")\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "\n",
        "#Combine the retriever with the compressor\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=retriever\n",
        ")\n",
        "\n",
        "# Create a QA chain with the compressed retriever\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=compression_retriever,\n",
        "    return_source_documents=True\n",
        ")"
      ],
      "metadata": {
        "id": "etyb9_9feY70"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example usage"
      ],
      "metadata": {
        "id": "Bw9qOhuxgGiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the main topic of the document?\"\n",
        "result = qa_chain.invoke({\"query\": query})\n",
        "print(result[\"result\"])\n",
        "print(\"Source documents:\", result[\"source_documents\"])"
      ],
      "metadata": {
        "id": "cnVUP3fQey_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81b95d20-0d4e-4c67-c511-4fdd28744af5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The main topic of the document is global and local climate action.\n",
            "Source documents: [Document(metadata={'source': 'data/Understanding_Climate_Change.pdf', 'page': 23}, page_content='The Kyoto Protocol, adopted in 1997, set binding emission reduction targets for developed countries. It was the first major international treaty to address climate change. The protocol laid the groundwork for subsequent agreements, highlighting the importance of collective action.  \\n\\nRegional and National Initiatives  \\n\\nEuropean Green Deal  \\n\\nThe European Green Deal is an ambitious plan to make Europe the first climate -neutral continent by 2050. It includes measures to reduce emissions, promote clean energy, and support sustainable agriculture and biodiversity. The deal also aims to create jobs  and  enhance economic resilience.'), Document(metadata={'source': 'data/Understanding_Climate_Change.pdf', 'page': 22}, page_content=\"negative emissions. The captured CO2 can be stored or used in various applications. Scaling DAC technology and reducing costs are critical for its widespread adoption.  \\nChapter 16: Global Cooperation and Governance  \\nInternational Agreements  \\nParis Agreement  \\nThe Paris Agreement is a landmark international accord that aims to limit global warming to \\nwell below 2 degrees Celsius above pre -industrial levels, with efforts to limit the increase to \\n1.5 degrees Celsius. Countries submit nationally determined contribu tions (NDCs) outlining \\ntheir climate action plans. Regular reviews and updates of NDCs are essential for meeting the \\nagreement's goals.\"), Document(metadata={'source': 'data/Understanding_Climate_Change.pdf', 'page': 30}, page_content='Chapter 21: Climate Change and Cultural Shifts\\nChanging Cultural Narratives\\nClimate Stories\\nStorytelling is a powerful tool for changing cultural narratives around climate change.\\nPersonal stories, documentaries, and artistic expressions can humanize climate impacts and\\ninspire action. Climate stories highlight the experiences of individuals and communities,\\nfostering empathy and connection.\\nCultural Movements'), Document(metadata={'source': 'data/Understanding_Climate_Change.pdf', 'page': 9}, page_content='Extracted relevant parts:\\n\\nChapter 6: Global and Local Climate Action  \\nInternational Collaboration  \\nUnited Nations Framework Convention on Climate Change (UNFCCC)  \\nThe UNFCCC is an international treaty aimed at addressing climate change. It provides a \\nframework for negotiating specific protocols and agreements, such as the Kyoto Protocol and \\nthe Paris Agreement. Global cooperation under the UNFCCC is crucial for coordinated \\nclimate action.  \\nParis Agreement  \\nThe Paris Agreement, adopted in 2015, aims to limit global warming to well below 2 degrees \\nCelsius above pre -industrial levels, with efforts to limit the increase to 1.5 degrees Celsius. \\nCountries submit nationally determined contributions (NDCs) outlining  their climate action \\nplans and targets.  \\nNational Strategies  \\nCarbon Pricing  \\nCarbon pricing mechanisms, such as carbon taxes and cap -and-trade systems, incentivize \\nemission reductions by assigning a cost to carbon emissions. These policies encourage')]\n"
          ]
        }
      ]
    }
  ]
}
